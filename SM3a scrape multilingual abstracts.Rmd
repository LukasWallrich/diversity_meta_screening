---
title: "Try to scrape abstracts for Google Scholar results"
output: html_notebook
---

Google Scholar results are drawn from many sources - here, the results for languages other than English are processed with attempts to retrieve abstracts from the resource links, to support the screening (Google Scholar provided snippets are very brief, so often unclear.)

# Identify where to find abstracts in most common sources

```{r}
unlist_nan <- function(x) {
  x[map_lgl(x, is.nan)] <- NA
  unlist(x)
}

language_results_df %>%  
  mutate(link = unlist_nan(link) %>% str_remove("#.*$")) %>% 
  filter(!str_detect(link, "\\.pdf$")) %>% 
  mutate(domain = link %>% str_remove(".*?\\.") %>% str_remove("/.*")) %>% 
  group_by(domain) %>% filter(n() >= 50) %>% slice_head(n=2) %>% 
  select(domain, link) %>% 
  gt::gt()

language_results_df %>%  
  mutate(link = unlist_nan(link) %>% str_remove("#.*$")) %>% 
  filter(str_detect(link, "\\.pdf$")) %>% 
  mutate(domain = link %>% str_remove(".*?\\.") %>% str_remove("/.*")) %>% 
  group_by(domain) %>% filter(n() >= 50) %>% slice_head(n=2) %>% 
  select(domain, link) %>% 
  gt::gt()

```
rivisteweb.it and torossa.com do not seem to have abstracts

For PDFs, academia.edu links appear broken, redalyc.org does not have abstracts

```{r}

library(rvest)

pdf_workarounds <- c("core.ac.uk", "ac.uk", "bibliotekanauki.pl", ".pl", "koreascience.or.kr", "researchgate.net", "springer.com")

abstracts_tags_specific <- tibble::tribble(
  ~target_domain, ~tag, ~class, ~note, 
  "cairn.info", "div", "resume lang-en", NA,
  "ceeol.com", "p", "summary", NA,
  "cqvip.com", "td", "sum", NA,
  "dbpia.co.kr", "div", "abstractTxt", NA,
  "jstage.jst.go.jp", "div", "article-overiew-abstract-wrap", "id, not class",
  "scielo.br", "article", "articleText", "id, not class",
  "repo.nii.ac.jp", "td", "td_detail_line_repos w80", "then get longest - these are all meta-data fields", #Some links there are also direct download links,
  "springer.com", "div", "Abs1-section", NA,
  "bibliotekanauki.pl", "p", "abstract-text preserve-whitespace", "if two exist, second likely to be the English one" ,
  "core.ac.uk", "section", "abstract", "id",
  "koreascience.org.kr", "div", "article-box", NA,
  "researchgate.net", "div", "section__abstract", NA,
  "sciencedirect.com", "div", "abstract author", "2nd likely English, if there are 2",
  "icm.edu.pl", "div", "articleDetails-abstract", "2nd likely English",
  "unirioja.es", "ul", "resumen", NA,
  "jstor.org", "p", "summary-paragraph", NA,
  "google.com",  "div", "synopsistext", "id"
)

abstract_names <- c("abstract", "resume")
frequent_classes <- c("div", "td", "p")

generic_selector <- map(frequent_classes, ~glue("{.x}[class*='{abstract_names}'], {.x}[id*='{abstract_names}']")) %>% 
  unlist() %>% paste(collapse = ", ")

uastring <- "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36"

httr::set_config(httr::user_agent(uastring))

scrape_abstract <- function(url) {
  url <- url %>% str_remove("#.*$")
  domain <- url %>% str_remove(".*?\\.") %>% str_remove("/.*")
    
if (str_detect(url, "\\.pdf$")) {
  if (!domain %in% pdf_workarounds) {
    return(tibble(url = url, domain = domain, skipped = "pdf"))
  } else {
    if (domain %in% c("bibliotekanauki.pl", "koreascience.or.kr", ".pl")) {
      url <- str_remove(url, "\\.pdf$")
    } else if (domain == "core.ac.uk" || domain == "ac.uk") {
      url <- str_replace(url, "/download/pdf/", "/display/") %>% 
        str_remove("\\.pdf$")
    } else if (domain == "researchgate.net") {
      url <- str_extract(url, "^.*publication/.*?/")
   } else if (domain == "springer.com") {
      url <- paste0("https://doi.org/", str_extract(url, "10\\..*$") %>% str_remove("\\.pdf"))
    }
    scrape_abstract(url)
  }
}
  
if (domain == "kjjb.org" & str_detect(url, "downloadArticleFile.do")) {
  # Transform download URLs into links to abstracts
  url <- url %>% str_extract("id=.*$") %>% str_remove("id=") %>% 
    {glue("http://www.kjjb.org/CN/abstract/abstract{.}.shtml")}
} else if (domain == "scielo.br") {
  url <- url %>% str_replace("/citation/", "/abstract/")
} else if (str_detect(url, "books.google.com")) {
  url <- url %>% str_remove("&oi.*")
}

if (str_detect(url, "create_pdf|downloadArticleFile\\.do|(bitstream.*\\.pdf)")) {
  cli::cli_alert_danger("Link appears to be a download link. Skipping.")
  cli::cli_alert_info(paste("URL: ", url))
  return(tibble(url = url, domain = domain, skipped = "pdf"))
}  

rate <- rate_backoff(pause_base = 4, max_times = 4)

  n <- Sys.time()
url_data <- (possibly(insistently(read_html, rate), NULL))(url)
  message(Sys.time() - n)

if (is.null(url_data)) {
                       cli::cli_alert_danger("URL could not be read. Skipping.")
                       cli::cli_alert_info(paste("URL: ", url))
                       return(tibble(url = url, domain = domain, skipped = "httperror"))

}

if (domain == "psych.ac.cn") {
  out <- html_nodes(url_data, "p") %>% html_text() %>% 
    stringi::stri_enc_toutf8() %>% str_subset("Abstract:") 
   return(tibble(url = url, domain = domain, abstract = out))
}

if (domain == "um.ac.id") {
  out <-  html_nodes(url_data, "div[id='articleAbstract']") %>% 
    html_text() %>% stringi::stri_enc_toutf8()
  if (length(out) > 0)    return(tibble(url = url, domain = domain, abstract = out))
  out <- html_nodes(url_data, "div[class='ep_summary_content_main']") %>% 
    html_nodes("p") %>% html_text() %>% stringi::stri_enc_toutf8()%>% .[2] 
   return(tibble(url = url, domain = domain, abstract = out))
}

if (domain == "upv.es") {
   out <- html_nodes(url_data, "tr") %>% html_text() %>% 
     stringi::stri_enc_toutf8()%>% str_subset("Abstract|Resumen") 
   return(tibble(url = url, domain = domain, abstract = out))
}

  different <- FALSE
  tentative <- FALSE

if (domain %in% abstracts_tags_specific$target_domain) {
  details <- abstracts_tags_specific %>% filter(target_domain == domain)
  selector <- glue::glue("{details$tag}[id*='{details$class}'], {details$tag}[class*='{details$class}']")
} else {
  different <- TRUE
  selector <- generic_selector
}
# Use CSS selectors to extract the text from the div
out <- div_text <- html_nodes(url_data, selector) %>%
  html_text() %>% stringi::stri_enc_toutf8() %>% 
  str_squish_mild() %>%  #Added after initial run!
  get_longest()

if (length(out) == 0 | is.na(out)) {
  # As last resort, find the longest paragraph on the page
  divs <- html_nodes(url_data, "div")
  texts <- divs[sapply(divs, function(x) length(html_nodes(x, "div")) == 0)] %>% 
  html_text() 
  
  out <- c(texts, html_nodes(url_data, "p, tr") %>% html_text()) %>% 
    stringi::stri_enc_toutf8() %>% 
    str_squish_mild() %>%  #Added after initial run!
    get_longest()
  
  tentative <- TRUE
}
tibble(url = url, domain = domain, abstract = out, 
       different_source = different, tentative_extraction = tentative)
}

plan(multisession, workers = 7)

# Randomly sort links to space out calls to pages in particular language
links <- language_results_df$link %>% sample()

scraped <- showing_progress(links[1:2], future_map, possibly(scrape_abstract, tibble()))
scraped2 <- showing_progress(links[21:1000], future_map, possibly(scrape_abstract, tibble()))
scraped3 <- showing_progress(links[1001:3000], future_map, possibly(scrape_abstract, tibble()))
scraped4 <- showing_progress(links[3001:5000], future_map, possibly(scrape_abstract, tibble()))
scraped5a <- showing_progress(links[5001:6000], future_map, possibly(scrape_abstract, tibble()))
scraped5b <- showing_progress(links[6001:7000], future_map, possibly(scrape_abstract, tibble()))
scraped6a <- showing_progress(links[7001:8000], future_map, possibly(scrape_abstract, tibble()))
scraped6b <- showing_progress(links[8001:9215], future_map, possibly(scrape_abstract, tibble()))

all <- c(scraped , scraped2 , scraped3 , scraped4 , scraped5a , scraped5b , scraped6a , scraped6b) 

all[map_lgl(all, ~not(is.data.frame(.x)))] <- list(tibble()) # One of the error functions returned tibble function, rather than tibble

scraped_df <- all %>% bind_rows()

scraped_df <- scraped_df %>% mutate(abstract = str_squish_mild(abstract))

# Made a mistake in processing pdf links - so trying those again

pdf_links <- scraped_pdf_links %>% filter(is.na(abstract)) %>% pull(url)

scraped_pdf_links <- showing_progress(pdf_links, map, scrape_abstract)
scraped_pdf_links <- scraped_pdf_links %>% bind_rows()

pdf_links_remaining <- setdiff(pdf_links, scraped_pdf_links %>% filter(!is.na(abstract)) %>% pull(url))

possibly2 <- function(.f, otherwise=NULL) {
  function(...) {
    tryCatch({
      .f(...)  
    }, error = function(e) otherwise(...))
  }
}

scraped_pdf_links2 <- showing_progress(pdf_links_remaining, future_map, possibly2(scrape_abstract, \(url) {
  message("Failed with ", url)
  return(tibble(url = url, skipped = "error", message = e))
}))

scraped_pdf_links2 <- scraped_pdf_links2 %>% bind_rows()

scraped_pdf_links4 <- map(failed_links, possibly2(scrape_abstract, \(url) {
  message("Failed with ", url)
  return(tibble(url = url, skipped = "error", message = list(e)))
}), .progress = TRUE)

scraped_pdf_links3 %>% bind_rows() %>% filter(skip = "httperror") %>% pull(url)

scraped_pdf_links3 %>% bind_rows() %>% filter(skipped == "httperror") %>% pull(url) -> failed_links

scraped_df <- bind_rows(scraped_pdf_links, scraped_pdf_links2, scraped_pdf_links3, scraped_pdf_links4, scraped_df) %>% 
  arrange(url, is.na(abstract)) %>% group_by(url) %>% slice_head(n = 1) %>% ungroup()


transform_link <- function(url) {
    url <- url %>% str_remove("#.*$")
  domain <- url %>% str_remove(".*?\\.") %>% str_remove("/.*")
if (str_detect(url, "\\.pdf$")) {
  if (!domain %in% pdf_workarounds) {
    return(url)
  } else {
    if (domain %in% c("bibliotekanauki.pl", "koreascience.or.kr", ".pl")) {
      url <- str_remove(url, "\\.pdf$")
    } else if (domain == "core.ac.uk" || domain == "ac.uk") {
      url <- str_replace(url, "/download/pdf/", "/display/") %>% 
        str_remove("\\.pdf$")
    } else if (domain == "researchgate.net") {
      url <- str_extract(url, "^.*publication/.*?/")
   } else if (domain == "springer.com") {
      url <- paste0("https://doi.org/", str_extract(url, "10\\..*$") %>% str_remove("\\.pdf"))
    }
  }
}

if (domain == "kjjb.org" & str_detect(url, "downloadArticleFile.do")) {
  # Transform download URLs into links to abstracts
  url <- url %>% str_extract("id=.*$") %>% str_remove("id=") %>% 
    {glue("http://www.kjjb.org/CN/abstract/abstract{.}.shtml")}
} else if (domain == "scielo.br") {
  url <- url %>% str_replace("/citation/", "/abstract/")
} else if (str_detect(url, "books.google.com")) {
  url <- url %>% str_remove("&oi.*")
}

url

}

language_results_df <- language_results_df %>% rowwise() %>% mutate(url = link %>% transform_link())


```
