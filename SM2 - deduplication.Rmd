---
title: "Deduplicate search results"
output: html_notebook
---

This process follows [the registration](https://osf.io/f5qdn) created after this meta-analysis was accepted in principle by the Journal of Business and Psychology.

# Packages used and set-up

```{r}
if(!require(pacman)) install.packages("pacman")
pacman::p_load(reticulate, tidyr, dplyr, jsonlite, purrr, stringr, rcrossref, bib2df, furrr,
               shiny, shinythemes, progress, progressr, keys, shinyjs, readr, synthesisr,
               rvest, glue)

pacman::p_load_gh("gadenbuie/shinyThings")

#Enable progress updates
#handlers(global = TRUE) <- this needs to be run in the console for now
 handlers(handler_progress(
     format   = ":spin :current/:total (:message) [:bar] :percent in :elapsed (remaining: :eta)",
     width    = 60,
     complete = "=",
     clear = FALSE
   ))

 options(progressr.clear = FALSE)

 
#Adds progress bars to map and future_map functions
showing_progress <- function(x, fn, fn2, ..., steps = length(x)) {
  if(interactive() && !handlers(global = NA)) stop("To use this function, first run `handlers(global = TRUE)` in the console")
   p <- progressr::progressor(steps = steps)
   p(amount = 0)
   fn(x, function(...) {
     out <- fn2(...)
     p()
     out
   })
}


set.seed(1234)

#Import
source("API_keys.R")
source("crossref_and_related.R") #Functions to retrieve and edit citations
source("helpers.R") #Helper functions - sometimes simple wrappers to deal safely with NULLs

source_python("get_scrape_functions.py")

library(magrittr)

```

# Import

```{r}
scholar_results <- read_rds("results_aligned/scholar_augmented.RDS") %>% mutate(doi = tolower(doi))
ssrn_results <- read_rds("results_aligned/ssrn_augmented.RDS")  %>% mutate(doi = tolower(doi))

psych_results <- read_rds("results_aligned/ebsco_psy_bs.RDS") %>% split(.$database)
business_source_results <- psych_results[[1]]  %>% mutate(doi = tolower(doi))
psych_results <- psych_results[[2]]  %>% mutate(doi = tolower(doi))

open_diss_results <- read_rds("results_aligned/ebsco_open_diss.RDS")
ndltd_results <- read_rds("results_aligned/ndltd_results.RDS")

open_citations_metas_results <- read_rds("results_aligned/open_citations_df.RDS")  %>% mutate(doi = tolower(doi))
scopus_citations_metas_results <- read_rds("results_aligned/scopus_citations_df.RDS")  %>% mutate(doi = tolower(doi)) #Note that scopus_id here is eid - so not sure if comparable to other scopus.

refs_previous_scopus_results <- read_rds("results_aligned/scopus_previous.RDS")  %>% mutate(doi = tolower(doi))
refs_previous_pdfs_results <- read_rds("results_aligned/grobid_refs.RDS")  %>% mutate(doi = tolower(doi))

```

# Number of results

```{r}
ls() %>% str_subset("results$") %>% map(\(current) {
  df <- get(current)
  if (!inherits(df, "data.frame")) return(data.frame())
  data.frame(source = str_remove(current, "_results"), hits = nrow(df))
}) %>% bind_rows() %>% arrange(-hits) %>%  gt::gt() %>% gt::summary_rows(columns = hits, fns = list(Total = ~sum(.)), decimals = 0)
```


# Internal deduplication

Before the overall deduplication, we will conduct some internal deduplications within the same search strategy - that increases transparency and efficiency.

## Google Scholar and SSRN internally

Separate queries in Google Scholar and SSRN will likely have returned duplicate hits.

```{r}
print(glue("Google Scholar searches returned {sum(duplicated(scholar_results$result_id))} duplicated results across the three queries"))
scholar_results <- scholar_results %>% group_by(result_id) %>% slice_head(n = 1) %>% ungroup()
```

```{r}
print(glue("SSRN searches returned {sum(duplicated(ssrn_results$result_id))} duplicated results across the two queries"))
ssrn_results <- ssrn_results %>% group_by(result_id) %>% slice_head(n = 1) %>% ungroup()

print(glue("GS searches returned {length(intersect(scholar_results$result_id, ssrn_results$result_id))} results also caught in SSRN searches"))
scholar_results <- scholar_results %>% filter(!result_id %in% ssrn_results$result_id)
```

## NDLTD

Since they draw on many repositories, substantial internal duplication is inevitable. Deduplication is not trivial, but *identical* titles, authors and years appear, or identical titles *and* abstracts can be safely removed. Note that authors are sometimes listed as Last, First and others as First Last - this is aligned here.

```{r}
invert_author <- (function(x) {
  if (str_count(x, ",") == 1) {
    last <- str_extract(x, "^.+?(?=,)")
    rest <- str_remove(x, "^.+, ")
    paste(rest, last)
  } else {
    x
  }
}) %>% Vectorize()

ndltd_results <- ndltd_results %>% mutate(identifier1 = paste0(clean_text(invert_author(author)), clean_text(title), clean_text(year)), identifier2 = paste0(clean_text(title), clean_text(abstract)))

print(glue("NDLTD searches returned (at least) {sum(duplicated(ndltd_results$identifier1) | duplicated(ndltd_results$identifier2))} internally duplicated results."))

first <- function(x) {
  x %>% na.omit() %>% .[1]
}

ndltd_results <- ndltd_results %>% 
  group_by(identifier1) %>% 
  summarise(across(c(database, title, author, year, type), first),
            across(c(abstract, english_abstract), get_longest),
            language = paste(unique(language), collapse = "; "),
            resource_link = list(resource_link),
            translate_narrow = translate_narrow[which.max(str_length(english_abstract))],
            .groups = "drop") %>% 
  mutate(identifier2 = paste0(clean_text(title), clean_text(abstract))) %>% 
  group_by(identifier2) %>% 
  summarise(across(c(database, title, author, year, type), first),
            across(c(abstract, english_abstract), get_longest),
            language = paste(unique(language), collapse = "; "),
            resource_link = list(resource_link),
            translate_narrow = translate_narrow[which.max(str_length(english_abstract))],
            .groups = "drop") %>% 
  rename(abstract_english = english_abstract)

```

They also have very broad stemming etc and returned many seemingly irrelevant terms. Therefore, search is repeated here.

```{r}
ndltd_results <- ndltd_results %>% mutate(identifier = paste(title, abstract, abstract_english), 
                                          hit = str_detect(identifier, "diverse|diversity|heterogenous|heterogeneity|(team composition)|(individual differences)") &
      str_detect(identifier, "team|group") & 
      str_detect(identifier, "performance|creativity|productivity|innovation|effectiveness"))

strings <- c("Interventions and Supports to Ameliorate Math Anxiety in K-12 Schools: A Meta-Analysis of Experimental Group Design Research", "Efetividade de dispersão por antas (Tapirus terrestris) : aspectos comportamentais de deposição de fezes e germinação de sementes", "Evaluating Horticultural Therapy Curriculum for Elderly, Chronic Psychiatric Illness, and Intellectual Disability / 高齡者、精神障礙者、心智障礙者之園藝治療課程評估")       

check_english <-  function(strings) {
  map_lgl(strings, \(x) {
      res <- cld2::detect_language_mixed(x)
      browser()
      "en" %in% res$code & res$proportion[res$code == "en"] > 0.2
  })
}

check_english(strings[1])
  
  
  
  res <- cld2::detect_language_mixed(x)


ndltd_results %>% filter(hit) %>% mutate(title_lang)

```


## Open Dissertations

For similar reasons, this is likely to contain duplicates - identified again by concatenating author, year and title.

```{r}
open_diss_results <- open_diss_results %>% mutate(identifier = paste(clean_text(title), clean_text(author), year))

print(glue("Open Dissertations returned (at least) {sum(duplicated(open_diss_results$identifier))} internally duplicated results."))

open_diss_results <- open_diss_results %>% arrange(identifier) %>% group_by(identifier) %>% slice_head(n = 1) %>% ungroup() %>% select(-identifier)
```


## References in meta-analyses and reviews

We included references from 15 previous meta-analyses and reviews - with substantial overlap.

```{r}
refs_previous_results <- refs_previous_pdfs_results %>%
  mutate(cited_in = str_remove(database, "references_"), database = "previous_reviews") %>% 
  bind_rows(refs_previous_scopus_results %>% mutate(across(where(is.list), unlist_w_NULLs)))

print(glue("Searches of previous references returned {sum((duplicated(refs_previous_results$doi) & !is.na(refs_previous_results$doi)) | (duplicated(refs_previous_results$scopus_id) & !is.na(refs_previous_results$scopus_id)))} duplicates (based on DOI and Scopus IDs only)"))

      
refs_previous_results <- refs_previous_results %>% filter(!is.na(scopus_id)) %>% group_by(scopus_id) %>% summarise(
  across(c(everything(), -cited_in, -in_scopus), get_longest),
  in_scopus = first(in_scopus),
  cited_in = paste(unique(cited_in), collapse = "; ")
) %>% 
  bind_rows(refs_previous_results %>% filter(is.na(scopus_id)))

refs_previous_results <- refs_previous_results %>% filter(!is.na(doi)) %>% group_by(doi) %>% summarise(
  across(c(everything(), -cited_in, -in_scopus), get_longest),
  in_scopus = first(in_scopus),
  cited_in = paste(unique(cited_in), collapse = "; ")
) %>% 
  bind_rows(refs_previous_results %>% filter(is.na(doi)))
```


## Citations of meta-analyses

OpenCitations and Scopus will likely have substantial overlap. For now, we compare that purely based on DOIs, which are included in all OpenCitation entries - some Scopus entries are missing them, so that overlap is underestimated.


```{r}
citation_metas_result <- scopus_citations_metas_results %>% 
  bind_rows_to_chr(open_citations_metas_results) %>% 
  rename(citing = cited_in) %>% 
  mutate(citing = tolower(citing))

citation_metas_result %>% 
  select(doi, citing, database) %>% 
  pivot_wider(names_from = database, values_from = database, values_fn = ~TRUE, values_fill = FALSE) %>% 
  group_by(citing) %>% 
  summarise(total = n(), 
            Scopus = sum(ScopusCitations_previous),
            OpenDissertations = sum(OpenCitations_previous),
            both = sum(ScopusCitations_previous & OpenCitations_previous), 
            only_Scopus = sum(ScopusCitations_previous & !OpenCitations_previous),
            only_OpenCitations = sum(!ScopusCitations_previous & OpenCitations_previous))

print(glue("Searches of citations of previous meta-analyses returned {sum((duplicated(citation_metas_result$doi) & !is.na(citation_metas_result$doi)) | (duplicated(citation_metas_result$scopus_id) & !is.na(citation_metas_result$scopus_id)))} duplicates (based on DOI and Scopus IDs only)"))


citation_metas_result <- citation_metas_result %>% filter(!is.na(scopus_id)) %>% group_by(scopus_id) %>% summarise(
  across(c(everything(), -citing, -database), get_longest),
  citing = paste(unique(citing), collapse = "; "),
  database = paste(unique(database), collapse = "; ")
) %>% 
  bind_rows(citation_metas_result %>% filter(is.na(scopus_id)))

citation_metas_result <- citation_metas_result %>% filter(!is.na(doi)) %>% group_by(doi) %>% summarise(
  across(c(everything(), -citing, -database), get_longest),
  citing = paste(unique(citing), collapse = "; "),
  database = paste(unique(database), collapse = "; ")
) %>% 
  bind_rows(citation_metas_result %>% filter(is.na(doi)))
```


```{r}
# Add missing abstracts - due to large duplication, were not requested from Scopus before
# Some doi led to 401 errors - therefore extracted with possibly
citation_metas_result$abstract[is.na(citation_metas_result$abstract) & citation_metas_result$database == "ScopusCitations_previous" & !is.na(citation_metas_result$doi)]  <-
  citation_metas_result$doi[is.na(citation_metas_result$abstract) & citation_metas_result$database == "ScopusCitations_previous" & !is.na(citation_metas_result$doi)]  %>% 
  showing_progress(map_chr, function(x) {
 possibly(get_scopus_abstract, otherwise = list("", ""))(x) %>% {get_longer(.[[1]], .[[2]])}})

# Search crossref D
job::job(title = "Add crossref abstract", {
  handlers(global = TRUE) 
handlers(handler_progress(
  format   = ":spin :current/:total (:message) [:bar] :percent in :elapsed (remaining: :eta)",
  width    = 60,
  complete = "=",
  clear = FALSE
))
  plan(multisession, workers = 5) #Parallel API requests to crossref
  citation_metas_result$abstract[is.na(citation_metas_result$abstract) & !is.na(citation_metas_result$doi)]  <-
  citation_metas_result$doi[is.na(citation_metas_result$abstract) & !is.na(citation_metas_result$doi)]  %>% 
  showing_progress(future_map_chr, function(x) {
 possibly(cr_abstract, otherwise = NA_character_)(x)})
})
  

```


## Check Google Scholar against Crossref

### For Scholar searches

```{r}

scholar_results <- scholar_results %>% tibble::rowid_to_column("id") %>% mutate(database = "GoogleScholar")

scholar_results_merge <-  
  scholar_results %>% transmute(
    id = id,
  doi = doi,  
  cr_match_scholar = cr_match %>% str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"),
  cr_match_cr = "", #To show only once
  title_scholar = title,
  title_cr = cr_title,
  author_scholar = coalesce(select(., matches("author.*name")) %>% reduce(paste, sep = ", ") %>% 
    str_remove_all(", NA,|NA,|, NA$| NA$") %>% 
    str_trim() %>% str_replace("^$", NA_character_), first_author),
  author_cr = collapse_author_df(author),
  year_scholar = year,
  year_cr = cr_year,
  abstract_scholar = snippet, 
  abstract_cr = abstract, 
  journal_scholar = str_extract(summary, "(?<= - ).*(?=, \\d)"), #Usually abbreviated
  journal_cr = journal
  )

parse_result <- function(match) {
  
match %>% clean_text() %>% str_remove_all("[a-z]{2,10}|\\||1") %>% 
    str_squish() %>% tibble(match = .) %>% separate(match, into = c("title_match", "year_match", "first_match"))
}

scholar_results_merge <- scholar_results_merge %>% mutate(parse_result(cr_match_scholar)) %>% 
  # Crossref comparison struggled with HTML code - now fixed, but need to recompute:
  mutate(title_match = if_else((clean_text(title_cr) == clean_text(title_scholar) |
           safe_str_detect(clean_text(title_cr), clean_text(title_scholar)) |
           safe_str_detect(clean_text(title_scholar), clean_text(title_cr))), "y", "n"),
         cr_match_scholar = glue::glue("title: {toupper(title_match)} | year: {toupper(year_match)} | 1st: {toupper(first_match)}") %>% 
           str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"))
  

source("screen_matches.R")

#No doi means no possible match has been returned
scholar_results_merge$result[is.na(scholar_results_merge$doi)] <- "no_match_delete"

#Matches on year, first author and title don't need to be reviewed when title is long
#given that substrings are accepted, shorter titles could be false positives
#but if journal is the same, then this is also good
scholar_results_merge$result[((str_count(scholar_results_merge$title_scholar, "\\S+") >= 6 &
                               str_count(scholar_results_merge$title_cr, "\\S+") >= 6) | 
                                (!is.na(scholar_results_merge$journal_scholar) & !is.na(scholar_results_merge$journal_cr) & 
                                   clean_text(scholar_results_merge$journal_scholar) == clean_text(scholar_results_merge$journal_cr))) &
                               map_int(str_extract_all(scholar_results_merge$cr_match_scholar, "Y"), 
                                       length)==3] <- "match"



scholar_results_merge$Ys <- lengths(str_extract_all(scholar_results_merge$cr_match_scholar, "Y"))


scholar_results_merge[is.na(scholar_results_merge$result),][1,] %>% glimpse()

scholar_results_merge <- screen_matches(scholar_results_merge %>% arrange(-Ys, title_match, year_match, first_match), suffixes = c("scholar", "cr"))

```

#### Merge results

```{r}
scholar_results_merge <- scholar_results %>% select(id, result_id, type, resource_link, citation_count, citation_link) %>% 
  right_join(scholar_results_merge) 

matched <- scholar_results_merge %>% filter(result == "match") %>%
  transmute(database = "Google Scholar",
         doi = doi,
         gs_result_id = result_id,
         author = get_longer(author_scholar, author_cr),
         year = year_scholar,
         title = get_longer(title_scholar, title_cr),
         abstract = get_longer(abstract_scholar, abstract_cr),
         pubtype = type,
         pubtitle = get_longer(journal_scholar, journal_cr),
         resource_link,
         citation_count
         ) %>% tibble()

no_match <- scholar_results_merge %>% filter(str_detect(result, "no_match")) %>%
  transmute(database = "Google Scholar",
         doi = NA,
         gs_result_id = result_id,
         author = author_scholar,
         year = year_scholar,
         title = title_scholar,
         abstract = abstract_scholar,
         pubtype = NA,
         pubtitle = journal_scholar,
         resource_link,
         citation_count,
         citation_link
         ) %>% tibble()

cr_addition <- scholar_results_merge %>% filter(str_detect(result, "no_match_keep")) %>%
  transmute(database = "Crossref",
         doi = doi,
         author = author_cr,
         year = year_cr,
         title = title_cr,
         abstract = abstract_cr,
         pubtype = if_else(str_detect(journal_cr, fixed("journal", ignore_case=TRUE)), "journal-article", NA_character_),
         pubtitle = journal_cr
         ) %>% tibble()

scholar_final <- bind_rows_to_chr(matched, no_match, cr_addition)
write_rds(scholar_final, "results_final/scholar_final.RDS")

```

### For SSRN searches

```{r}
ssrn_results <- ssrn_results %>% tibble::rowid_to_column("id") %>% mutate(database = "SSRN")

ssrn_results_merge <-  
  ssrn_results %>% transmute(
    id = id,
  doi = doi,  
  cr_match_ssrn = cr_match %>% str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"),
  cr_match_cr = "", #To show only once
  title_ssrn = title,
  title_cr = cr_title,
  author_ssrn = coalesce(select(., matches("author.*name")) %>% reduce(paste, sep = ", ") %>% 
    str_remove_all(", NA,|NA,|, NA$| NA$") %>% 
    str_trim() %>% str_replace("^$", NA_character_), first_author),
  author_cr = collapse_author_df(author),
  year_ssrn = year,
  year_cr = cr_year,
  abstract_ssrn = snippet, 
  abstract_cr = abstract, 
  journal_ssrn = str_extract(summary, "(?<= - ).*(?=, \\d)"), #Usually abbreviated
  journal_cr = journal
  )

ssrn_results_merge <- ssrn_results_merge %>% mutate(parse_result(cr_match_ssrn)) %>% 
  # Crossref comparison struggled with HTML code - now fixed, but need to recompute:
  mutate(title_match = if_else((clean_text(title_cr) == clean_text(title_ssrn) |
           safe_str_detect(clean_text(title_cr), clean_text(title_ssrn)) |
           safe_str_detect(clean_text(title_ssrn), clean_text(title_cr))), "y", "n"),
         cr_match_ssrn = glue::glue("title: {toupper(title_match)} | year: {toupper(year_match)} | 1st: {toupper(first_match)}") %>% 
           str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"))
  
source("screen_matches.R")

ssrn_results_merge$result[is.na(ssrn_results_merge$doi)] <- "no_match_delete"

#Matches on year, first author and title don't need to be reviewed when title is long
#given that substrings are accepted, shorter titles could be false positives
ssrn_results_merge$result[str_count(ssrn_results_merge$title_ssrn, "\\S+") >= 6 &
                               str_count(ssrn_results_merge$title_cr, "\\S+") >= 6 &
                               map_int(str_extract_all(ssrn_results_merge$cr_match_ssrn, "Y"), 
                                       length)==3] <- "match"

#Sort by quality of matches so that "default" screening answer is consistent
ssrn_results_merge$Ys <- lengths(str_extract_all(ssrn_results_merge$cr_match_ssrn, "Y"))

ssrn_results_merge <- screen_matches(ssrn_results_merge %>% arrange(-Ys, title_match, year_match, first_match), suffixes = c("ssrn", "cr"))

```

#### Merge results

```{r}
ssrn_results_merge <- ssrn_results %>% 
  select(id, result_id, type, resource_link, citation_count, citation_link) %>% 
  right_join(ssrn_results_merge) 

matched <- ssrn_results_merge %>% filter(result == "match") %>%
  transmute(database = "SSRN",
         doi,
         gs_result_id = result_id,
         author = get_longer(author_ssrn, author_cr),
         year = year_ssrn,
         title = get_longer(title_ssrn, title_cr),
         abstract = get_longer(abstract_ssrn, abstract_cr),
         pubtype = type,
         pubtitle = coalesce(journal_ssrn, journal_cr),
         resource_link,
         citation_count
         ) %>% tibble()

no_match <- ssrn_results_merge %>% filter(str_detect(result, "no_match")) %>%
  transmute(database = "SSRN",
         doi = NA,
         gs_result_id = result_id,
         author = author_ssrn,
         year = year_ssrn,
         title = title_ssrn,
         abstract = abstract_ssrn,
         pubtype = NA,
         pubtitle = journal_ssrn,
         resource_link,
         citation_count,
         citation_link
         ) %>% tibble()

cr_addition <- ssrn_results_merge %>% filter(str_detect(result, "no_match_keep")) %>%
  transmute(database = "Crossref",
         doi = doi,
         author = author_cr,
         year = year_cr,
         title = title_cr,
         abstract = abstract_cr,
         pubtype = if_else(str_detect(journal_cr, fixed("journal", ignore_case=TRUE)), "journal-article", NA_character_),
         pubtitle = journal_cr,
         ) %>% tibble()

ssrn_final <- bind_rows_to_chr(matched, no_match, cr_addition)
write_rds(ssrn_final, "results_final/ssrn_final.RDS")

```

## Merge results and remove duplicated DOIs

- DOIs are treated as definitive indices of duplication.

```{r}

full_results <- list(
  scholar_final,
  ssrn_final,
  psych_results,
  business_source_results,
  citation_metas_result,
  refs_previous_results,
  ndltd_results,
  open_diss_results
)

# Ensure that first has all columns - required for type conversion during merge
full_results[[1]][map(full_results, names) %>% unlist() %>% unique() %>% setdiff(names(full_results[[1]]))] <- NA

full_results <- full_results %>% 
  do.call(bind_rows_to_chr, .) %>% 
  mutate(record_id = as.character(row_number()))

# Merge columns
full_results <- full_results %>% mutate(
  type = case_when(type == "JOUR" ~ "journal-article",
                   type == "THES" ~ "thesis_dissertation",
                   type == "CHAP" ~ "chapter",
                   type == "BOOK" ~ "book",
                   type == "thesis_dissertation" ~ "thesis_dissertation",
                   is.na(type) ~ NA_character_,
                   TRUE ~ "other"),
  pubtype = case_when(pubtype == "journal-article" ~ "journal-article",
                   pubtype == "Article" ~ "journal-article",
                   pubtype == "THES" ~ "thesis_dissertation",
                   pubtype == "book-chapter" ~ "chapter",
                   pubtype == "Book Chapter" ~ "chapter",
                   pubtype == "book" ~ "book",
                   pubtype == "Book" ~ "book",
                   pubtype == "dissertation" ~ "thesis_dissertation",
                   pubtype == "Review" ~ "review_article",
                   is.na(pubtype) ~ NA_character_,
                   TRUE ~ "other"),
  pub_type = coalesce(type, pubtype)) %>% 
  unite(col = "journal",  journal, pubtitle, na.rm=TRUE, sep = ": ") %>%
  transmute(record_id, database, title, author, year, journal, abstract_original = abstract,
            abstract = coalesce(abstract_english, abstract),
            doi, pub_type, database_detail = coalesce(cited_in, citing),
            volume, issue, start_page, end_page, publisher,
                        resource_link,
                        language, gs_result_id, scopus_id, citation_count, citation_link
                        ) 

full_results <- full_results %>% mutate(
  doi = na_if(doi, ""),
  citation_count = list(tibble(source = database, citation_count = citation_count) %>%
    filter(!is.na(citation_count))),
  resource_link = list(tibble(source = database, resource_link = resource_link) %>%
    filter(!is.na(resource_link)))
)

full_results_no_doi <- full_results %>%
  filter(is.na(doi))
  

select_year <- function(...) {
  #Look for numeric year (in case one source was n.d.)
  years <- list(...) %>% unlist() %>% na.omit()
  if (length(years) == 0) return(NA_character_)
  years_num <- years %>% as.numeric() %>% coalesce()
  #Remove years that are invalid - more than 1 in the future, and pre-1700
  years_num[(years_num>as.integer(format(Sys.Date(), "%Y"))+1) | years_num < 1700] <- NA
  if(!is.na(coalesce(!!!years_num))) return(as.character(coalesce(!!!years_num)))
  coalesce(!!!years)
}

full_results_doi <- full_results %>%
  filter(!is.na(doi)) %T>%
  {total_hits_with_doi <<- nrow(.)} %>%
  group_by(doi) %>%
  summarise(across(c(author, title, abstract, pubtype, pubtitle, journal), get_longest),
            across(c(citation_link, issue, volume, start_page, end_page, publisher, type), first),
            year = select_year(year),
            citation_count = list(bind_rows(citation_count)),
            resource_link = list(bind_rows(resource_link)),
            database = glue::glue_collapse(database, sep = "; "),
            record_id = glue::glue_collapse(record_id, sep = "; ")) %>% 
  ungroup()

print(glue::glue("Removed {total_hits_with_doi - nrow(full_results_doi)} results based on duplicate DOIs."))

full_results_post_doi <- bind_rows(full_results_no_doi, full_results_doi)

```

## Deduplicate other entries

```{r}
pacman::p_load_gh("camaradesuk/ASySD")


hits_dedup <- hits_post_doi %>% rename(number = issue) %>% 
  mutate(pages = paste(start_page, end_page, sep = "-"), isbn = NA, label = NA, source = database) %>%
  dedup_citations()

hits_dedup %>% write_rds("automatic_dedup_done.Rds")

```

### Manually review possible duplicates

```{r}
source("manual_dedup.R")

hits_dedup$manual_dedup <- hits_dedup$manual_dedup %>% rename_with(~str_replace(.x, "1$", "_1")) %>% 
  rename_with(~str_replace(.x, "2$", "_2")) %>% manual_dedup()

# Code merging of those identified as duplicates
manual_matches <- hits_dedup$manual_dedup %>% filter(result == "match") %>% 
  mutate(new_duplicate_id = (row_number() + nrow(hits)) %>% as.character()) %>% 
  select(new_duplicate_id, record_id_1, record_id_2) %>% 
  pivot_longer(2:3, names_to = NULL, values_to = "record_id")

hits_dedup$unique <- hits_dedup$unique %>% left_join(manual_matches)

already_unique <- hits_dedup$unique %>% filter(is.na(new_duplicate_id)) %>% 
    rename(issue = number)

newly_unique <- hits_dedup$unique %>% filter(!is.na(new_duplicate_id)) %>% 
  rename(issue = number) %>% 
  group_by(new_duplicate_id) %>% 
  summarise(across(c(author, title, abstract, pubtype, pubtitle, journal), get_longest),
            across(c(citation_link, issue, volume, start_page, end_page, publisher, source_type), ~na.omit(unique(.x))[1]),
            year = select_year(year),
            citation_count = list(tibble(source = database, citation_count = citation_count) %>%
                                    filter(!is.na(citation_count))),
            resource_link = list(tibble(source = database, resource_link = resource_link) %>%
                                    filter(!is.na(resource_link))),
            database = glue::glue_collapse(database, sep = "; "),
            rowid = glue::glue_collapse(record_id, sep = "; ")) %>% 
  ungroup()

deduplicated_hits <- bind_rows(already_unique, newly_unique)

write_rds(deduplicated_hits, "deduplicated_hits.Rds")
```


## Searches in other languages

### Run Google Scholar search per language

For instance in German:

("Kontakthypothese" OR "Inter* Kontakt" OR Interkultureller Kontakt“) AND ("Arbeit" ODER "Arbeitsplatz") AND ("Vorurteil" OR "Einstellungen")

```{r}
pacman::p_load(googleLanguageR)
gl_auth("google_translate.json")
translate <- function(x, drop_tags = TRUE) {
  if (drop_tags) {
    x <- str_replace_all(x, "<.*?>", " ")
  }
  which_NA <- is.na(x)
  x <- c(na.omit(x))
  t <- gl_translate(x)
  t %>%
    count(detectedSourceLanguage) %>%
    transmute(N = paste0(detectedSourceLanguage, ": ", n)) %>%
    pull() %>%
    glue::glue_collapse(sep = ", ", last = "&") %>%
    message("Detected languages: ", .)
  out <- character(length(which_NA))
  out[!which_NA] <- t %>% pull(translatedText)
  out[which_NA] <- NA
  out
}

language_queries <- c(
  de = '("Kontakthypothese" OR "Inter* Kontakt" OR Interkultureller Kontakt“) AND ("Arbeit" ODER "Arbeitsplatz") AND ("Vorurteil" OR "Einstellungen")'
)

for (i in seq_along(language_queries)) {
  language_results <- get_scholar_results(
    q = language_queries[1],
    max_results = 60, serp_key = serp_key
  ) # Change to greater max_results in production

  language_results <- language_results %>%
    unnest_wider(publication_info) %>%
    select(-type) %>%
    unnest_wider(authors, names_sep = "_", names_repair = "universal") %>%
    unnest_wider(matches("authors_[0-9]+$"), names_sep = "_", names_repair = "unique") %>% # Unnests all columns matching the term
    mutate(
      year = str_extract(summary, "[0-9]{4}"),
      first_author = str_remove(summary, ",.*$") %>%
        str_remove("-.*$") %>% str_remove("^[A-Z]* ")
    ) %>%
    hoist(resources, resource_link = c(1, 3)) %>%
    hoist(inline_links, citation_count = c("cited_by", "total"), citation_link = "serpapi_cite_link")

  # Workaround for pmap as list columns cause problems - https://github.com/tidyverse/purrr/issues/846
  ls_cols <- language_results %>% keep(is.list)
  other_cols <- language_results %>% keep(~ !(is.list(.x)))

  plan(multisession, workers = 5) # Parallel API requests to crossref

  aug_df <- other_cols %>%
    showing_progress(function(...) future_pmap_dfr(..., .options = furrr_options(seed = TRUE)), function(...) {
      df <- extract_crossref(tibble(...))
      # if ("doi" %in% names(df) && !is.na(df$doi)) df <- get_citation(df)
      df
    }, steps = nrow(.)) %>%
    cbind(ls_cols)

  aug_df$language <- names(language_queries)[1]


  aug_df$Ys <- lengths(str_extract_all(aug_df$cr_match, "Y"))

  aug_df[aug_df$Ys == 0, "result"] <- "no_match_delete"

  message("Translating ", names(language_queries)[1], " results")

  aug_df[c("title", "snippet")] <- map_dfr(aug_df[c("title", "snippet")], translate)
  aug_df[aug_df$Ys != 0, c("abstract", "cr_title")] <- map_dfr(aug_df[aug_df$Ys != 0, c("abstract", "cr_title")], translate)

  save_rds(aug_df, glue::glue("results_aligned/scholar_{names(language_queries)[1]}.RDS"))
}
```

From there:

- run crossref manual merge as per GS and SSRN above
- do title and snippet screening to decide which abstracts to retrieve
- retrieve abstracts from links (prob with BeautifulSoup)
- do full screening