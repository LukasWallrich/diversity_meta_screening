---
title: "Deduplicate search results"
output: html_notebook
---

This process follows [the registration](https://osf.io/f5qdn) created after this meta-analysis was accepted in principle by the Journal of Business and Psychology.

# Packages used and set-up

```{r}
if(!require(pacman)) install.packages("pacman")
pacman::p_load(reticulate, tidyr, dplyr, jsonlite, purrr, stringr, rcrossref, bib2df, furrr,
               shiny, shinythemes, progress, progressr, keys, shinyjs, readr, synthesisr,
               rvest, glue)

pacman::p_load_gh("gadenbuie/shinyThings")

#Enable progress updates
#handlers(global = TRUE) <- this needs to be run in the console for now
 handlers(handler_progress(
     format   = ":spin :current/:total (:message) [:bar] :percent in :elapsed (remaining: :eta)",
     width    = 60,
     complete = "=",
     clear = FALSE
   ))

 options(progressr.clear = FALSE)

 
#Adds progress bars to map and future_map functions
showing_progress <- function(x, fn, fn2, ..., steps = length(x)) {
  if(interactive() && !handlers(global = NA)) stop("To use this function, first run `handlers(global = TRUE)` in the console")
   p <- progressr::progressor(steps = steps)
   p(amount = 0)
   fn(x, function(...) {
     out <- fn2(...)
     p()
     out
   })
}


set.seed(1234)

#Import
source("API_keys.R")
source("crossref_and_related.R") #Functions to retrieve and edit citations
source("helpers.R") #Helper functions - sometimes simple wrappers to deal safely with NULLs

source_python("get_scrape_functions.py")

library(magrittr)

```

# Import

```{r}
scholar_results <- read_rds("results_aligned/scholar_augmented.RDS") %>% mutate(doi = tolower(doi))
ssrn_results <- read_rds("results_aligned/ssrn_augmented.RDS")  %>% mutate(doi = tolower(doi))

psych_results <- read_rds("results_aligned/ebsco_psy_bs.RDS") %>% split(.$database)
business_source_results <- psych_results[[1]]  %>% mutate(doi = tolower(doi))
psych_results <- psych_results[[2]]  %>% mutate(doi = tolower(doi))

open_diss_results <- read_rds("results_aligned/ebsco_open_diss.RDS")
ndltd_results <- read_rds("results_aligned/ndltd_results.RDS")

open_citations_metas_results <- read_rds("results_aligned/open_citations_df.RDS")  %>% mutate(doi = tolower(doi))
scopus_citations_metas_results <- read_rds("results_aligned/scopus_citations_df.RDS")  %>% mutate(doi = tolower(doi)) #Note that scopus_id here is eid - so not sure if comparable to other scopus.

refs_previous_scopus_results <- read_rds("results_aligned/scopus_previous.RDS")  %>% mutate(doi = tolower(doi))
refs_previous_pdfs_results <- read_rds("results_aligned/grobid_refs.RDS")  %>% mutate(doi = tolower(doi))

```

# Number of results

```{r}
ls() %>% str_subset("results$") %>% map(\(current) {
  df <- get(current)
  if (!inherits(df, "data.frame")) return(data.frame())
  data.frame(source = str_remove(current, "_results"), hits = nrow(df))
}) %>% bind_rows() %>% arrange(-hits) %>%  gt::gt() %>% gt::grand_summary_rows(columns = hits, fns = list(label = md("**Total**"), fn = "sum"), decimals = 0)
```


# Internal deduplication

Before the overall deduplication, we will conduct some internal deduplications within the same search strategy - that increases transparency and efficiency.

## Google Scholar and SSRN internally

Separate queries in Google Scholar and SSRN will likely have returned duplicate hits.

```{r}
print(glue("Google Scholar searches returned {sum(duplicated(scholar_results$result_id))} duplicated results across the three queries"))
scholar_results <- scholar_results %>% group_by(result_id) %>% slice_head(n = 1) %>% ungroup()
```

```{r}
print(glue("SSRN searches returned {sum(duplicated(ssrn_results$result_id))} duplicated results across the two queries"))
ssrn_results <- ssrn_results %>% group_by(result_id) %>% slice_head(n = 1) %>% ungroup()

print(glue("GS searches returned {length(intersect(scholar_results$result_id, ssrn_results$result_id))} results also caught in SSRN searches"))
scholar_results <- scholar_results %>% filter(!result_id %in% ssrn_results$result_id)
```

## NDLTD

Since they draw on many repositories, substantial internal duplication is inevitable. Deduplication is not trivial, but *identical* titles, authors and years, or identical titles *and* abstracts can be safely removed. Note that authors are sometimes listed as Last, First and others as First Last - this is aligned here.

```{r}
invert_author <- (function(x) {
  if (str_count(x, ",") == 1) {
    last <- str_extract(x, "^.+?(?=,)")
    rest <- str_remove(x, "^.+, ")
    paste(rest, last)
  } else {
    x
  }
}) %>% Vectorize()

ndltd_results <- ndltd_results %>% 
  mutate(identifier1 = paste0(clean_text(invert_author(author)), clean_text(title), clean_text(year)), 
         identifier2 = paste0(clean_text(title), clean_text(abstract)))

print(glue("NDLTD searches returned (at least) {sum(duplicated(ndltd_results$identifier1) | duplicated(ndltd_results$identifier2))} internally duplicated results."))

duplicate_identifiers <- c(ndltd_results$identifier1[duplicated(ndltd_results$identifier1)],
                           ndltd_results$identifier2[duplicated(ndltd_results$identifier2)])

first <- function(x) {
  x %>% na.omit() %>% .[1]
}

ndltd_results <- ndltd_results %>% 
  group_by(identifier1) %>% 
  summarise(across(c(database, title, author, year, type), first),
            across(c(abstract, english_abstract), get_longest),
            language = paste(unique(language), collapse = "; "),
            resource_link = list(resource_link),
            translate_narrow = translate_narrow[which.max(str_length(english_abstract))],
            .groups = "drop") %>% 
  mutate(identifier2 = paste0(clean_text(title), clean_text(abstract))) %>% 
  group_by(identifier2) %>% 
  summarise(across(c(database, title, author, year, type), first),
            across(c(abstract, english_abstract), get_longest),
            language = paste(unique(language), collapse = "; "),
            resource_link = list(resource_link),
            translate_narrow = translate_narrow[which.max(str_length(english_abstract))],
            .groups = "drop") %>% 
  rename(abstract_english = english_abstract)

```

They also have very broad stemming etc and returned many seemingly irrelevant terms. Therefore, search is repeated here.

```{r}
ndltd_results <- ndltd_results %>% mutate(identifier = paste(title, abstract, abstract_english), 
                                          hit = str_detect(identifier, "diverse|diversity|heterogenous|heterogeneity|(team composition)|(individual differences)") &
      str_detect(identifier, "team|group") & 
      str_detect(identifier, "performance|creativity|productivity|innovation|effectiveness"))

print(glue("NDLTD searches returned {sum(!(ndltd_results$hit))} results that did not actually contain the search terms in title or abstract. These were excluded."))

ndltd_results <- ndltd_results %>% filter(hit == TRUE) %>%   
  # Restore for reporting
  mutate(identifier1 = paste0(clean_text(invert_author(author)), clean_text(title), clean_text(year)))

print(glue("For the remaining {sum(ndltd_results$hit)} results, {sum(duplicate_identifiers %in% ndltd_results$identifier2 | duplicate_identifiers %in% ndltd_results$identifier1)} internal duplicates had already been removed."))

```



## Open Dissertations

For similar reasons, this is likely to contain duplicates - identified again by concatenating author, year and title.

```{r}
open_diss_results <- open_diss_results %>% mutate(identifier = paste(clean_text(title), clean_text(author), year))

print(glue("Open Dissertations returned (at least) {sum(duplicated(open_diss_results$identifier))} internally duplicated results."))

open_diss_results <- open_diss_results %>% arrange(identifier) %>% group_by(identifier) %>% slice_head(n = 1) %>% ungroup() %>% select(-identifier)
```


## References in meta-analyses and reviews

We included references from 15 previous meta-analyses and reviews - with substantial overlap.

```{r}
refs_previous_results <- refs_previous_pdfs_results %>%
  mutate(cited_in = str_remove(database, "references_"), database = "previous_reviews") %>% 
  bind_rows(refs_previous_scopus_results %>% mutate(across(where(is.list), unlist_w_NULLs)))

print(glue("Searches of previous references returned {sum((duplicated(refs_previous_results$doi) & !is.na(refs_previous_results$doi)) | (duplicated(refs_previous_results$scopus_id) & !is.na(refs_previous_results$scopus_id)))} duplicates (based on DOI and Scopus IDs only)"))

      
refs_previous_results <- refs_previous_results %>% filter(!is.na(scopus_id)) %>% 
  group_by(scopus_id) %>% summarise(
  across(c(everything(), -cited_in, -in_scopus), get_longest),
  in_scopus = first(in_scopus),
  cited_in = paste(unique(cited_in), collapse = "; ")
) %>% 
  bind_rows(refs_previous_results %>% filter(is.na(scopus_id)))

refs_previous_results <- refs_previous_results %>% filter(!is.na(doi)) %>% 
  group_by(doi) %>% summarise(
  across(c(everything(), -cited_in, -in_scopus), get_longest),
  in_scopus = first(in_scopus),
  cited_in = paste(unique(cited_in), collapse = "; ")
) %>% 
  bind_rows(refs_previous_results %>% filter(is.na(doi)))
```


## Citations of meta-analyses

OpenCitations and Scopus will likely have substantial overlap. For now, we compare that purely based on DOIs, which are included in all OpenCitation entries - some Scopus entries are missing them, so that overlap is underestimated.


```{r}
citation_metas_result <- scopus_citations_metas_results %>% 
  bind_rows_to_chr(open_citations_metas_results) %>% 
  rename(citing = cited_in) %>% 
  mutate(citing = tolower(citing))

citation_metas_result %>% 
  select(doi, citing, database) %>% 
  pivot_wider(names_from = database, values_from = database, values_fn = ~TRUE, values_fill = FALSE) %>% 
  group_by(citing) %>% 
  summarise(total = n(), 
            Scopus = sum(ScopusCitations_previous),
            OpenDissertations = sum(OpenCitations_previous),
            both = sum(ScopusCitations_previous & OpenCitations_previous), 
            only_Scopus = sum(ScopusCitations_previous & !OpenCitations_previous),
            only_OpenCitations = sum(!ScopusCitations_previous & OpenCitations_previous))

print(glue("Searches of citations of previous meta-analyses returned {sum((duplicated(citation_metas_result$doi) & !is.na(citation_metas_result$doi)) | (duplicated(citation_metas_result$scopus_id) & !is.na(citation_metas_result$scopus_id)))} duplicates (based on DOI and Scopus IDs only)"))


citation_metas_result <- citation_metas_result %>% filter(!is.na(scopus_id)) %>% group_by(scopus_id) %>% summarise(
  across(c(everything(), -citing, -database), get_longest),
  citing = paste(unique(citing), collapse = "; "),
  database = paste(unique(database), collapse = "; ")
) %>% 
  bind_rows(citation_metas_result %>% filter(is.na(scopus_id)))

citation_metas_result <- citation_metas_result %>% filter(!is.na(doi)) %>% group_by(doi) %>% summarise(
  across(c(everything(), -citing, -database), get_longest),
  citing = paste(unique(citing), collapse = "; "),
  database = paste(unique(database), collapse = "; ")
) %>% 
  bind_rows(citation_metas_result %>% filter(is.na(doi)))
```


```{r}

if (interactive()) {

    # Add missing abstracts - due to large duplication, were not requested from Scopus before
    # Some doi led to 401 errors - therefore extracted with possibly
    citation_metas_result$abstract[is.na(citation_metas_result$abstract) & citation_metas_result$database == "ScopusCitations_previous" & !is.na(citation_metas_result$doi)]  <-
      citation_metas_result$doi[is.na(citation_metas_result$abstract) & citation_metas_result$database == "ScopusCitations_previous" & !is.na(citation_metas_result$doi)]  %>% 
      showing_progress(map_chr, function(x) {
     possibly(get_scopus_abstract, otherwise = list("", ""))(x) %>% {get_longer(.[[1]], .[[2]])}})
    
    # Search crossref API
    job::job(title = "Add crossref abstract", {
      handlers(global = TRUE) 
    handlers(handler_progress(
      format   = ":spin :current/:total (:message) [:bar] :percent in :elapsed (remaining: :eta)",
      width    = 60,
      complete = "=",
      clear = FALSE
    ))
      plan(multisession, workers = 5) #Parallel API requests to crossref
      citation_metas_result$abstract[is.na(citation_metas_result$abstract) & !is.na(citation_metas_result$doi)]  <-
      citation_metas_result$doi[is.na(citation_metas_result$abstract) & !is.na(citation_metas_result$doi)]  %>% 
      showing_progress(future_map_chr, function(x) {
     possibly(cr_abstract, otherwise = NA_character_)(x)})
    })

    qs::qsave(citation_metas_result, "results_aligned/citation_metas_result_dedup_abstracts.qs")
      
} else {
  citation_metas_result <- qs::qread("results_aligned/citation_metas_result_dedup_abstracts.qs")
}

```


## Check Google Scholar against Crossref

### For Scholar searches

```{r}

if (interactive()) {

scholar_results <- scholar_results %>% tibble::rowid_to_column("id") %>% mutate(database = "GoogleScholar")

scholar_results_merge <-  
  scholar_results %>% transmute(
    id = id,
  doi = doi,  
  cr_match_scholar = cr_match %>% str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"),
  cr_match_cr = "", #To show only once
  title_scholar = title,
  title_cr = cr_title,
  author_scholar = coalesce(select(., matches("author.*name")) %>% reduce(paste, sep = ", ") %>% 
    str_remove_all(", NA,|NA,|, NA$| NA$") %>% 
    str_trim() %>% str_replace("^$", NA_character_), first_author),
  author_cr = collapse_author_df(author),
  year_scholar = year,
  year_cr = cr_year,
  abstract_scholar = snippet, 
  abstract_cr = abstract, 
  journal_scholar = str_extract(summary, "(?<= - ).*(?=, \\d)"), #Usually abbreviated
  journal_cr = journal
  )

parse_result <- function(match) {
  
match %>% clean_text() %>% str_remove_all("[a-z]{2,10}|\\||1") %>% 
    str_squish() %>% tibble(match = .) %>% separate(match, into = c("title_match", "year_match", "first_match"))
}

scholar_results_merge <- scholar_results_merge %>% mutate(parse_result(cr_match_scholar)) %>% 
  # Crossref comparison struggled with HTML code - now fixed, but need to recompute:
  mutate(title_match = if_else((clean_text(title_cr) == clean_text(title_scholar) |
           safe_str_detect(clean_text(title_cr), clean_text(title_scholar)) |
           safe_str_detect(clean_text(title_scholar), clean_text(title_cr))), "y", "n"),
         cr_match_scholar = glue::glue("title: {toupper(title_match)} | year: {toupper(year_match)} | 1st: {toupper(first_match)}") %>% 
           str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"))
  

source("screen_matches.R")

#No doi means no possible match has been returned
scholar_results_merge$result[is.na(scholar_results_merge$doi)] <- "no_match_delete"

#Matches on year, first author and title don't need to be reviewed when title is long
#given that substrings are accepted, shorter titles could be false positives
#but if journal is the same, then this is also good
scholar_results_merge$result[((str_count(scholar_results_merge$title_scholar, "\\S+") >= 6 &
                               str_count(scholar_results_merge$title_cr, "\\S+") >= 6) | 
                                (!is.na(scholar_results_merge$journal_scholar) & !is.na(scholar_results_merge$journal_cr) & 
                                   clean_text(scholar_results_merge$journal_scholar) == clean_text(scholar_results_merge$journal_cr))) &
                               map_int(str_extract_all(scholar_results_merge$cr_match_scholar, "Y"), 
                                       length)==3] <- "match"



scholar_results_merge$Ys <- lengths(str_extract_all(scholar_results_merge$cr_match_scholar, "Y"))


scholar_results_merge[is.na(scholar_results_merge$result),][1,] %>% glimpse()

scholar_results_merge <- screen_matches(scholar_results_merge %>% arrange(-Ys, title_match, year_match, first_match), suffixes = c("scholar", "cr"))

}

```

#### Merge results

```{r}

if (interactive()) {
scholar_results_merge <- scholar_results %>% select(id, result_id, type, resource_link, citation_count, citation_link) %>% 
  right_join(scholar_results_merge) 

matched <- scholar_results_merge %>% filter(result == "match") %>%
  transmute(database = "Google Scholar",
         doi = doi,
         gs_result_id = result_id,
         author = get_longer(author_scholar, author_cr),
         year = year_scholar,
         title = get_longer(title_scholar, title_cr),
         abstract = get_longer(abstract_scholar, abstract_cr),
         pubtype = type,
         pubtitle = get_longer(journal_scholar, journal_cr),
         resource_link,
         citation_count
         ) %>% tibble()

no_match <- scholar_results_merge %>% filter(str_detect(result, "no_match")) %>%
  transmute(database = "Google Scholar",
         doi = NA,
         gs_result_id = result_id,
         author = author_scholar,
         year = year_scholar,
         title = title_scholar,
         abstract = abstract_scholar,
         pubtype = NA,
         pubtitle = journal_scholar,
         resource_link,
         citation_count,
         citation_link
         ) %>% tibble()

cr_addition <- scholar_results_merge %>% filter(str_detect(result, "no_match_keep")) %>%
  transmute(database = "Crossref",
         doi = doi,
         author = author_cr,
         year = year_cr,
         title = title_cr,
         abstract = abstract_cr,
         pubtype = if_else(str_detect(journal_cr, fixed("journal", ignore_case=TRUE)), "journal-article", NA_character_),
         pubtitle = journal_cr
         ) %>% tibble()

scholar_final <- bind_rows_to_chr(matched, no_match, cr_addition)
write_rds(scholar_final, "results_final/scholar_final.RDS")

} else {
  scholar_final <- read_rds("results_final/scholar_final.RDS")
}

```

### For SSRN searches

```{r}

if (interactive()) {
ssrn_results <- ssrn_results %>% tibble::rowid_to_column("id") %>% mutate(database = "SSRN")

ssrn_results_merge <-  
  ssrn_results %>% transmute(
    id = id,
  doi = doi,  
  cr_match_ssrn = cr_match %>% str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"),
  cr_match_cr = "", #To show only once
  title_ssrn = title,
  title_cr = cr_title,
  author_ssrn = coalesce(select(., matches("author.*name")) %>% reduce(paste, sep = ", ") %>% 
    str_remove_all(", NA,|NA,|, NA$| NA$") %>% 
    str_trim() %>% str_replace("^$", NA_character_), first_author),
  author_cr = collapse_author_df(author),
  year_ssrn = year,
  year_cr = cr_year,
  abstract_ssrn = snippet, 
  abstract_cr = abstract, 
  journal_ssrn = str_extract(summary, "(?<= - ).*(?=, \\d)"), #Usually abbreviated
  journal_cr = journal
  )

ssrn_results_merge <- ssrn_results_merge %>% mutate(parse_result(cr_match_ssrn)) %>% 
  # Crossref comparison struggled with HTML code - now fixed, but need to recompute:
  mutate(title_match = if_else((clean_text(title_cr) == clean_text(title_ssrn) |
           safe_str_detect(clean_text(title_cr), clean_text(title_ssrn)) |
           safe_str_detect(clean_text(title_ssrn), clean_text(title_cr))), "y", "n"),
         cr_match_ssrn = glue::glue("title: {toupper(title_match)} | year: {toupper(year_match)} | 1st: {toupper(first_match)}") %>% 
           str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"))
  
source("screen_matches.R")

ssrn_results_merge$result[is.na(ssrn_results_merge$doi)] <- "no_match_delete"

#Matches on year, first author and title don't need to be reviewed when title is long
#given that substrings are accepted, shorter titles could be false positives
ssrn_results_merge$result[str_count(ssrn_results_merge$title_ssrn, "\\S+") >= 6 &
                               str_count(ssrn_results_merge$title_cr, "\\S+") >= 6 &
                               map_int(str_extract_all(ssrn_results_merge$cr_match_ssrn, "Y"), 
                                       length)==3] <- "match"

#Sort by quality of matches so that "default" screening answer is consistent
ssrn_results_merge$Ys <- lengths(str_extract_all(ssrn_results_merge$cr_match_ssrn, "Y"))

ssrn_results_merge <- screen_matches(ssrn_results_merge %>% arrange(-Ys, title_match, year_match, first_match), suffixes = c("ssrn", "cr"))
}
```

#### Merge results

```{r}
if (interactive()) {

ssrn_results_merge <- ssrn_results %>% 
  select(id, result_id, type, resource_link, citation_count, citation_link) %>% 
  right_join(ssrn_results_merge) 

matched <- ssrn_results_merge %>% filter(result == "match") %>%
  transmute(database = "SSRN",
         doi,
         gs_result_id = result_id,
         author = get_longer(author_ssrn, author_cr),
         year = year_ssrn,
         title = get_longer(title_ssrn, title_cr),
         abstract = get_longer(abstract_ssrn, abstract_cr),
         pubtype = type,
         pubtitle = coalesce(journal_ssrn, journal_cr),
         resource_link,
         citation_count
         ) %>% tibble()

no_match <- ssrn_results_merge %>% filter(str_detect(result, "no_match")) %>%
  transmute(database = "SSRN",
         doi = NA,
         gs_result_id = result_id,
         author = author_ssrn,
         year = year_ssrn,
         title = title_ssrn,
         abstract = abstract_ssrn,
         pubtype = NA,
         pubtitle = journal_ssrn,
         resource_link,
         citation_count,
         citation_link
         ) %>% tibble()

cr_addition <- ssrn_results_merge %>% filter(str_detect(result, "no_match_keep")) %>%
  transmute(database = "Crossref",
         doi = doi,
         author = author_cr,
         year = year_cr,
         title = title_cr,
         abstract = abstract_cr,
         pubtype = if_else(str_detect(journal_cr, fixed("journal", ignore_case=TRUE)), "journal-article", NA_character_),
         pubtitle = journal_cr,
         ) %>% tibble()

ssrn_final <- bind_rows_to_chr(matched, no_match, cr_addition)
write_rds(ssrn_final, "results_final/ssrn_final.RDS")
} else {
  ssrn_final <- read_rds("results_final/ssrn_final.RDS")
}

```

## Merge results and remove duplicated DOIs

- DOIs are treated as definitive indices of duplication.

```{r}

full_results <- list(
  scholar_final,
  ssrn_final,
  psych_results,
  business_source_results,
  citation_metas_result,
  refs_previous_results,
  ndltd_results,
  open_diss_results
)

full_results <- qs::qread("results_final/full_results_list.qs")

# Ensure that first has all columns - required for type conversion during merge
full_results[[1]][map(full_results, names) %>% unlist() %>% unique() %>% setdiff(names(full_results[[1]]))] <- NA

full_results <- full_results %>% 
  do.call(bind_rows_to_chr, .) %>% 
  mutate(record_id = as.character(row_number()))

# Merge columns
full_results <- full_results %>% mutate(
  type = case_when(type == "JOUR" ~ "journal-article",
                   type == "THES" ~ "thesis_dissertation",
                   type == "CHAP" ~ "chapter",
                   type == "BOOK" ~ "book",
                   type == "thesis_dissertation" ~ "thesis_dissertation",
                   is.na(type) ~ NA_character_,
                   TRUE ~ "other"),
  pubtype = case_when(pubtype == "journal-article" ~ "journal-article",
                   pubtype == "Article" ~ "journal-article",
                   pubtype == "THES" ~ "thesis_dissertation",
                   pubtype == "book-chapter" ~ "chapter",
                   pubtype == "Book Chapter" ~ "chapter",
                   pubtype == "book" ~ "book",
                   pubtype == "Book" ~ "book",
                   pubtype == "dissertation" ~ "thesis_dissertation",
                   pubtype == "Review" ~ "review_article",
                   is.na(pubtype) ~ NA_character_,
                   TRUE ~ "other"),
  pub_type = coalesce(type, pubtype)) %>% 
  unite(col = "journal",  journal, pubtitle, na.rm=TRUE, sep = ": ") %>%
  transmute(record_id, database, title, author, year, journal, abstract_original = abstract,
            abstract = coalesce(abstract_english, abstract),
            doi, pub_type, database_detail = coalesce(cited_in, citing),
            volume, issue, start_page, end_page, publisher,
                        resource_link,
                        language, gs_result_id, scopus_id, citation_count, citation_link
                        ) 

full_results <- full_results %>% 
rowwise()  %>%
mutate(
  doi = na_if(doi, ""),
  citation_count = list(tibble(source = database, citation_count = citation_count) %>%
    filter(!is.na(citation_count))),
  resource_link = list(tibble(source = database, resource_link = resource_link) %>%
    filter(!is.na(resource_link)))
) %>% ungroup()

full_results_no_doi <- full_results %>%
  filter(is.na(doi))
  

select_year <- function(...) {
  #Look for numeric year (in case one source was n.d.)
  years <- list(...) %>% unlist() %>% na.omit()
  if (length(years) == 0) return(NA_character_)
  years_num <- years %>% as.numeric() %>% coalesce()
  #Remove years that are invalid - more than 1 in the future, and pre-1700
  years_num[(years_num>as.integer(format(Sys.Date(), "%Y"))+1) | years_num < 1700] <- NA
  if(!is.na(coalesce(!!!years_num))) return(as.character(coalesce(!!!years_num)))
  coalesce(!!!years)
}

full_results_doi <- full_results %>%
  filter(!is.na(doi)) %T>%
  {total_hits_with_doi <<- nrow(.)} %>%
  group_by(doi) %>%
  summarise(across(c(author, title, abstract, pub_type, journal), get_longest),
            across(c(citation_link, issue, volume, start_page, end_page, publisher), first),
            year = select_year(year),
            citation_count = list(bind_rows(citation_count)),
            resource_link = list(bind_rows(resource_link)),
            database = glue::glue_collapse(database, sep = "; "),
            record_id = glue::glue_collapse(record_id, sep = "; ")) %>% 
  ungroup()

print(glue::glue("Removed {total_hits_with_doi - nrow(full_results_doi)} results based on duplicate DOIs."))

full_results_post_doi <- bind_rows(full_results_no_doi, full_results_doi)

qs::qsave(full_results_post_doi, "results_final/full_results_post_doi_deduplication_df.qs")

```

## Deduplicate other entries

```{r}
if (interactive()) {
pacman::p_load_gh("camaradesuk/ASySD")

hits_dedup <- full_results_post_doi %>% rename(number = issue) %>% 
  mutate(pages = paste(start_page, end_page, sep = "-"), isbn = NA, label = NA, source = database) %>%
  dedup_citations()

hits_dedup %>% qs::qsave("results_final/automatic_dedup_done.qs")
} else {
hits_dedup <- qs::qread("results_final/automatic_dedup_done.qs")
}
```

### Manually review possible duplicates

```{r}
if (interactive()) {
source("manual_dedup.R")

manual_dedup_res <- hits_dedup$manual_dedup %>% rename_with(~str_replace(.x, "1$", "_1")) %>% 
  rename_with(~str_replace(.x, "2$", "_2")) %>% manual_dedup()

# To continue after a break
# manual_dedup_res <- manual_dedup_res %>% manual_dedup()

 manual_dedup_res %>% write_rds("manual_dedup_df_done.Rds")

} else {

 manual_dedup_res <- read_rds("manual_dedup_df_done.Rds")

}

# Code merging of those identified as duplicates
manual_matches <- manual_dedup_res %>% filter(result == "match") %>% 
  mutate(new_duplicate_id = (row_number() + nrow(full_results)) %>% as.character()) %>% 
  select(new_duplicate_id, record_id_1, record_id_2) %>% 
  pivot_longer(2:3, names_to = NULL, values_to = "record_id")

hits_dedup$unique <- hits_dedup$unique %>% left_join(manual_matches)

already_unique <- hits_dedup$unique %>% filter(is.na(new_duplicate_id)) %>% 
    rename(issue = number)

newly_unique <- hits_dedup$unique %>% filter(!is.na(new_duplicate_id)) %>% 
  rename(issue = number) %>% 
  group_by(new_duplicate_id) %>% 
  summarise(across(c(author, title, abstract, pub_type, journal), get_longest),
            across(c(citation_link, issue, volume, start_page, end_page, publisher), ~na.omit(unique(.x))[1]),
            year = select_year(year),
            citation_count = list(tibble(source = database, citation_count = citation_count) %>%
                                    filter(!is.na(citation_count))),
            resource_link = list(tibble(source = database, resource_link = resource_link) %>%
                                    filter(!is.na(resource_link))),
            source = glue::glue_collapse(database, sep = "; "),
            rowid = glue::glue_collapse(record_id, sep = "; ")) %>% 
  ungroup()

deduplicated_hits <- bind_rows(already_unique, newly_unique) %>% ungroup()

deduplicated_hits <- deduplicated_hits %>% mutate(record_id = coalesce(duplicate_id, new_duplicate_id)) %>% 
  select(-duplicate_id, -new_duplicate_id)

# Deduplicate once more to remove duplicates between manual and automatic deduplication
deduplicated_hits <- deduplicated_hits %>% rename(number = issue) %>% 
  dedup_citations()

deduplicated_hits$unique %>% rename(issue = number) %>% qs::qsave("results_final/final_deduplicated_results.qs")
```


### Create ASReview input filter

Here, some initial text cleaning was conducted to remove copyright notices - more of that might have increased AI accuracy further, to be considered in future work.

```{r}
deduplicated_hits <- qs::qread("results_final/final_deduplicated_results.qs")

# Clean up text 
deduplicated_hits <- deduplicated_hits %>%
  #remove tags
  mutate(abstract = str_remove_all(abstract, "<[a-zA-Z/][^>]*>"),
         title = str_remove_all(title, "<[a-zA-Z/][^>]*>"),
         #remove copyright notices
         abstract = str_remove(abstract, "\\[ABSTRACT\\sFROM\\sAUTHOR\\]\\sCopyright.*$"))

deduplicated_hits %>% 
  select(duplicate_id, title, abstract, doi, author) %>%
  write_csv("results_final/asreview_input.csv")
```
