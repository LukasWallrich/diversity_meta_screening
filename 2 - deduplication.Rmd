---
title: "Deduplicate search results and report on search"
output:
  html_document:
    code_folding: hide
---

This process follows [the registration](https://osf.io/f5qdn) created after this meta-analysis was accepted in principle by the Journal of Business and Psychology.

# Packages used and set-up

```{r}
if (!require(pacman)) install.packages("pacman")
pacman::p_load(
  reticulate, tidyr, dplyr, jsonlite, purrr, stringr, rcrossref, bib2df, furrr,
  shiny, shinythemes, progress, progressr, keys, shinyjs, readr, synthesisr,
  rvest, glue
)

pacman::p_load_gh("gadenbuie/shinyThings")

# Enable progress updates
# handlers(global = TRUE) <- this needs to be run in the console for now
handlers(handler_progress(
  format = ":spin :current/:total (:message) [:bar] :percent in :elapsed (remaining: :eta)",
  width = 60,
  complete = "=",
  clear = FALSE
))

options(progressr.clear = FALSE)


# Adds progress bars to map and future_map functions
showing_progress <- function(x, fn, fn2, ..., steps = length(x)) {
  if (interactive() && !handlers(global = NA)) stop("To use this function, first run `handlers(global = TRUE)` in the console")
  p <- progressr::progressor(steps = steps)
  p(amount = 0)
  fn(x, function(...) {
    out <- fn2(...)
    p()
    out
  })
}


set.seed(1234)

#Import
source("API_keys.R")
source("helper-code/crossref_and_related.R") #Functions to retrieve and edit citations
source("helper-code/helpers.R") #Helper functions - sometimes simple wrappers to deal safely with NULLs

source_python("helper-code/get_scrape_functions.py")

library(magrittr)
```

# Import

DOI are  not case-sensitive, but inconsistenly listed in databases - therefore, we convert them to lowercase for deduplication.

```{r}
scholar_results <- read_rds("results_aligned/scholar_augmented.RDS") %>% mutate(doi = tolower(doi))
ssrn_results <- read_rds("results_aligned/ssrn_augmented.RDS") %>% mutate(doi = tolower(doi))

psych_results <- read_rds("results_aligned/ebsco_psy_bs.RDS") %>% split(.$database)
business_source_results <- psych_results[[1]] %>% mutate(doi = tolower(doi))
psych_results <- psych_results[[2]] %>% mutate(doi = tolower(doi))

open_diss_results <- read_rds("results_aligned/ebsco_open_diss.RDS")
ndltd_results <- read_rds("results_aligned/ndltd_results.RDS")

open_citations_metas_results <- read_rds("results_aligned/open_citations_df.RDS") %>% mutate(doi = tolower(doi))
scopus_citations_metas_results <- read_rds("results_aligned/scopus_citations_df.RDS") %>% mutate(doi = tolower(doi))

refs_previous_scopus_results <- read_rds("results_aligned/scopus_previous.RDS") %>% mutate(doi = tolower(doi))
refs_previous_pdfs_results <- read_rds("results_aligned/grobid_refs.RDS") %>% mutate(doi = tolower(doi))
```

# Number of results

```{r}
prisma_res <- ls() %>%
  str_subset("results$") %>%
  map(\(current) {
    df <- get(current)
    if (!inherits(df, "data.frame")) {
      return(data.frame())
    }
    data.frame(source = str_remove(current, "_results"), total_hits = nrow(df))
  }) %>%
  bind_rows() %>%
  arrange(-total_hits)


prisma_res %>%
  gt::gt() %>%
  gt::grand_summary_rows(columns = total_hits, fns = list(Total = ~ sum(.)), decimals = 0)
```

# Internal deduplication

Before the overall deduplication, we will conduct some internal deduplications within the same search strategy - that increases transparency and efficiency.

## Google Scholar and SSRN internally

Separate queries in Google Scholar and SSRN will likely have returned duplicate hits.

```{r}
gs_internal_duplicates <- sum(duplicated(scholar_results$result_id))
print(glue("Google Scholar searches returned {gs_internal_duplicates} duplicated results across the three queries"))
scholar_results <- scholar_results %>%
  group_by(result_id) %>%
  slice_head(n = 1) %>%
  ungroup()
```

```{r}
ssrn_internal_duplicates <- sum(duplicated(ssrn_results$result_id))
print(glue("SSRN searches returned {ssrn_internal_duplicates} duplicated results across the two queries"))
ssrn_results <- ssrn_results %>%
  group_by(result_id) %>%
  slice_head(n = 1) %>%
  ungroup()

gs_ssrn_overlap <- length(intersect(scholar_results$result_id, ssrn_results$result_id))
print(glue("GS searches returned {gs_ssrn_overlap} results also caught in SSRN searches"))
scholar_results <- scholar_results %>% filter(!result_id %in% ssrn_results$result_id)
```

## NDLTD

They also have very broad stemming etc and returned many seemingly irrelevant terms. Therefore, search is repeated here.

```{r}
ndltd_results <- ndltd_results %>% mutate(
  identifier = paste(title, abstract, english_abstract),
  hit = str_detect(identifier, "diverse|diversity|heterogenous|heterogeneity|(team composition)|(individual differences)") &
    str_detect(identifier, "team|group") &
    str_detect(identifier, "performance|creativity|productivity|innovation|effectiveness")
)

ndltd_irrelevant <- nrow(ndltd_results) - sum(ndltd_results$hit)
print(glue("NDLTD searches returned {ndltd_irrelevant} seemingly irrelevant results - excluded here."))
ndltd_results <- ndltd_results %>% filter(hit)

prisma_res$total_hits[prisma_res$source == "ndltd"] <- nrow(ndltd_results)
```

Since they draw on many repositories, substantial internal duplication is inevitable. Deduplication is not trivial, but *identical* titles, authors and years appear, or identical titles *and* abstracts can be safely removed. Note that authors are sometimes listed as Last, First and others as First Last - this is aligned here.

```{r}
invert_author <- (function(x) {
  if (str_count(x, ",") == 1) {
    last <- str_extract(x, "^.+?(?=,)")
    rest <- str_remove(x, "^.+, ")
    paste(rest, last)
  } else {
    x
  }
}) %>% Vectorize()

ndltd_results <- ndltd_results %>% mutate(
  identifier1 = paste0(clean_text(invert_author(author)), clean_text(title), clean_text(year)),
  identifier2 = paste0(clean_text(title), clean_text(abstract)))

ndltd_int_dup <- sum(duplicated(ndltd_results$identifier1) | duplicated(ndltd_results$identifier2))

print(glue("NDLTD searches returned (at least) {ndltd_int_dup} internally duplicated results."))

first <- function(x) {
  x %>%
    na.omit() %>%
    .[1]
}

ndltd_results <- ndltd_results %>%
  group_by(identifier1) %>%
  summarise(across(c(database, title, author, year, type), first),
    across(c(abstract, english_abstract), get_longest),
    language = paste(unique(language), collapse = "; "),
    resource_link = list(resource_link),
    translate_narrow = translate_narrow[which.max(str_length(english_abstract))],
    .groups = "drop"
  ) %>%
  mutate(identifier2 = paste0(clean_text(title), clean_text(abstract))) %>%
  group_by(identifier2) %>%
  summarise(across(c(database, title, author, year, type), first),
    across(c(abstract, english_abstract), get_longest),
    language = paste(unique(language), collapse = "; "),
    resource_link = list(resource_link),
    translate_narrow = translate_narrow[which.max(str_length(english_abstract))],
    .groups = "drop"
  ) %>%
  rename(abstract_english = english_abstract)
```

## Open Dissertations

For similar reasons, this is likely to contain duplicates - identified again by concatenating author, year and title.

```{r}
open_diss_results <- open_diss_results %>% mutate(identifier = paste(clean_text(title), clean_text(author), year))

open_diss_dupl <- sum(duplicated(open_diss_results$identifier))

print(glue("Open Dissertations returned (at least) {open_diss_dupl} internally duplicated results."))

open_diss_results <- open_diss_results %>%
  arrange(identifier) %>%
  group_by(identifier) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  select(-identifier)
```

## References in meta-analyses and reviews

We included references from 15 previous meta-analyses and reviews - with substantial overlap.

```{r}
refs_previous_results <- refs_previous_pdfs_results %>%
  mutate(cited_in = str_remove(database, "references_"), database = "previous_reviews") %>%
  bind_rows(refs_previous_scopus_results %>% mutate(across(where(is.list), unlist_w_NULLs)))

prev_refs_duplication <- sum((duplicated(refs_previous_results$doi) & !is.na(refs_previous_results$doi)) | duplicated(refs_previous_results$scopus_id) & !is.na(refs_previous_results$scopus_id))

print(glue("Searches of previous references returned {prev_refs_duplication} duplicates (based on DOI and Scopus IDs only)"))

refs_previous_results <- refs_previous_results %>%
  filter(!is.na(scopus_id)) %>%
  group_by(scopus_id) %>%
  summarise(
    across(c(everything(), -cited_in, -in_scopus), get_longest),
    in_scopus = first(in_scopus),
    cited_in = paste(unique(cited_in), collapse = "; ")
  ) %>%
  bind_rows(refs_previous_results %>% filter(is.na(scopus_id)))

refs_previous_results <- refs_previous_results %>%
  filter(!is.na(doi)) %>%
  group_by(doi) %>%
  summarise(
    across(c(everything(), -cited_in, -in_scopus), get_longest),
    in_scopus = first(in_scopus),
    cited_in = paste(unique(cited_in), collapse = "; ")
  ) %>%
  bind_rows(refs_previous_results %>% filter(is.na(doi)))
```

## Citations of meta-analyses

OpenCitations and Scopus will likely have substantial overlap. For now, we compare that purely based on DOIs, which are included in all OpenCitation entries - some Scopus entries are missing them, so that overlap is underestimated.

```{r}
citation_metas_result <- scopus_citations_metas_results %>%
  bind_rows_to_chr(open_citations_metas_results) %>%
  rename(citing = cited_in) %>%
  mutate(citing = tolower(citing))

citation_metas_result %>%
  select(doi, citing, database) %>%
  pivot_wider(names_from = database, values_from = database, values_fn = ~TRUE, values_fill = FALSE) %>%
  group_by(citing) %>%
  summarise(
    total = n(),
    Scopus = sum(ScopusCitations_previous),
    OpenDissertations = sum(OpenCitations_previous),
    both = sum(ScopusCitations_previous & OpenCitations_previous),
    only_Scopus = sum(ScopusCitations_previous & !OpenCitations_previous),
    only_OpenCitations = sum(!ScopusCitations_previous & OpenCitations_previous)
  )

citation_metas_dupl <- sum((duplicated(citation_metas_result$doi) & !is.na(citation_metas_result$doi)) | (duplicated(citation_metas_result$scopus_id) & !is.na(citation_metas_result$scopus_id)))

print(glue("Searches of citations of previous meta-analyses returned {citation_metas_dupl} duplicates (based on DOI and Scopus IDs only)"))

citation_metas_result <- citation_metas_result %>%
  filter(!is.na(scopus_id)) %>%
  group_by(scopus_id) %>%
  summarise(
    across(c(everything(), -citing, -database), get_longest),
    citing = paste(unique(citing), collapse = "; "),
    database = paste(unique(database), collapse = "; ")
  ) %>%
  bind_rows(citation_metas_result %>% filter(is.na(scopus_id)))

citation_metas_result <- citation_metas_result %>%
  filter(!is.na(doi)) %>%
  group_by(doi) %>%
  summarise(
    across(c(everything(), -citing, -database), get_longest),
    citing = paste(unique(citing), collapse = "; "),
    database = paste(unique(database), collapse = "; ")
  ) %>%
  bind_rows(citation_metas_result %>% filter(is.na(doi)))
```

### Database duplicate removal

```{r}
psych_duplicates <- sum(duplicated(na.omit(psych_results$doi)))
print(glue("PsychInfo searches returned {psych_duplicates} duplicated results."))

psych_results <- psych_results %>%
  filter(!is.na(doi)) %>%
  group_by(doi) %>%
  summarise(
    across(everything(), get_longest)
  ) %>%
  bind_rows(psych_results %>% filter(is.na(doi)))

business_source_duplicates <- sum(duplicated(na.omit(business_source_results$doi)))
print(glue("Business Source Premier searches returned {business_source_duplicates} duplicated results."))

business_source_results <- business_source_results %>%
  filter(!is.na(doi)) %>%
  group_by(doi) %>%
  summarise(
    across(everything(), get_longest)
  ) %>%
  bind_rows(business_source_results %>% filter(is.na(doi)))

```

### PRISMA reporting

From here, extensive manual augmentation was required. This cannot be rerun dynamically, so the PRISMA reporting is conducted separately, partly from file.

```{r}
prisma_res <- tibble::tribble(
  ~source,                   ~internal_duplicated, 
   "ndltd",                   ndltd_int_dup, # Excluded ndltd_irrelevant results before proceeding     
   "psych",                   psych_duplicates,      
   "business_source",         business_source_duplicates,       
   "scopus_citations_metas",  citation_metas_dupl, # Together with OpenCitations      
   "open_citations_metas",    NA,       
   "open_diss",               open_diss_dupl,       
   "refs_previous_scopus",    prev_refs_duplication,  # Together with PDF refs     
   "scholar",                 gs_internal_duplicates + gs_ssrn_overlap, # Includes overlap with SSRN-focused GS searches       
   "ssrn",                    ssrn_internal_duplicates,       
   "refs_previous_pdfs",      NA
) %>% left_join(prisma_res, by = "source") 

prisma_res %>%
  gt::gt() %>%
  gt::grand_summary_rows(c(total_hits, internal_duplicated), fns = list(Total = ~ sum(.)), decimals = 0)

# Remove duplicate DOIs

all_dois <- list(
  scholar_results,
  ssrn_results,
  psych_results,
  business_source_results,
  citation_metas_result,
  refs_previous_results,
  ndltd_results,
  open_diss_results
) %>%
  map(~ if("doi" %in% colnames(.x)) .x %>% select(doi) %>% filter(!is.na(doi)) else tibble(doi = character(0))) %>%
  bind_rows()

n <- list(
  scholar_results,
  ssrn_results,
  psych_results,
  business_source_results,
  citation_metas_result,
  refs_previous_results,
  ndltd_results,
  open_diss_results
) %>%
  map_int(~ nrow(.x)) %>% sum()


dupl_ids <- sum(duplicated(all_dois$doi))

hits_dedup <- read_rds("results_dedup/automatic_dedup_done.Rds")

deduplicated_hits <- read_rds("results_dedup/deduplicated_hits.Rds")


deduplication <- tribble(
  ~step, ~duplicates,
  "Internal duplication*", sum(prisma_res$internal_duplicated, na.rm = TRUE),
  "Cross-database DOI de-duplication", dupl_ids,
  "ASYSD (automatic and manual)", sum(prisma_res$total_hits) - sum(prisma_res$internal_duplicated, na.rm = TRUE) - dupl_ids - nrow(deduplicated_hits)
)

prisma_res %>%
  rowwise() %>%
  mutate(net = sum(total_hits, -internal_duplicated, na.rm = TRUE)) %>%
  gt::gt() %>%
  gt::grand_summary_rows(c(total_hits, internal_duplicated), fns = list(Total = ~ sum(.)), decimals = 0)


deduplication %>%
  gt::gt() %>%
  gt::grand_summary_rows(c(duplicates), fns = list(Total = ~ sum(.)), decimals = 0)


print(glue::glue("After deduplication, the number of unique hits was {nrow(deduplicated_hits)}."))

```


### Continued deduplication

```{r}
# Add missing abstracts - due to large duplication, were not requested from Scopus before
# Some doi led to 401 errors - therefore extracted with possibly
citation_metas_result$abstract[is.na(citation_metas_result$abstract) & citation_metas_result$database == "ScopusCitations_previous" & !is.na(citation_metas_result$doi)] <-
  citation_metas_result$doi[is.na(citation_metas_result$abstract) & citation_metas_result$database == "ScopusCitations_previous" & !is.na(citation_metas_result$doi)] %>%
  showing_progress(map_chr, function(x) {
    possibly(get_scopus_abstract, otherwise = list("", ""))(x) %>%
      {
        get_longer(.[[1]], .[[2]])
      }
  })

# Search crossref for remaining abstracts
job::job(title = "Add crossref abstract", {
  handlers(global = TRUE)
  handlers(handler_progress(
    format = ":spin :current/:total (:message) [:bar] :percent in :elapsed (remaining: :eta)",
    width = 60,
    complete = "=",
    clear = FALSE
  ))
  plan(multisession, workers = 5) # Parallel API requests to crossref
  citation_metas_result$abstract[is.na(citation_metas_result$abstract) & !is.na(citation_metas_result$doi)] <-
    citation_metas_result$doi[is.na(citation_metas_result$abstract) & !is.na(citation_metas_result$doi)] %>%
    showing_progress(future_map_chr, function(x) {
      possibly(cr_abstract, otherwise = NA_character_)(x)
    })
})
```


## Check Google Scholar (GS) against Crossref

So far, Crossref results are saved alongside GS results. We now check for matches, to see which GS results can be superseded by more comprehensive Crossref results, and where Crossref might have returned additional results. For this, we use an interface adapted from [revtools](https://github.com/mjwestgate/revtools) package.

### For Scholar searches

```{r}
scholar_results <- scholar_results %>%
  tibble::rowid_to_column("id") %>%
  mutate(database = "GoogleScholar")

scholar_results_merge <-
  scholar_results %>% transmute(
    id = id,
    doi = doi,
    cr_match_scholar = cr_match %>% str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"),
    cr_match_cr = "", # To show only once
    title_scholar = title,
    title_cr = cr_title,
    author_scholar = coalesce(select(., matches("author.*name")) %>%
      reduce(paste, sep = ", ") %>%
      str_remove_all(", NA,|NA,|, NA$| NA$") %>%
      str_trim() %>%
      str_replace("^$", NA_character_), first_author),
    author_cr = collapse_author_df(author),
    year_scholar = year,
    year_cr = cr_year,
    abstract_scholar = snippet,
    abstract_cr = abstract,
    journal_scholar = str_extract(summary, "(?<= - ).*(?=, \\d)"), # Usually abbreviated
    journal_cr = journal
  )

parse_result <- function(match) {
  match %>%
    clean_text() %>%
    str_remove_all("[a-z]{2,10}|\\||1") %>%
    str_squish() %>%
    tibble(match = .) %>%
    separate(match, into = c("title_match", "year_match", "first_match"))
}

scholar_results_merge <- scholar_results_merge %>%
  mutate(parse_result(cr_match_scholar)) %>%
  mutate(
    title_match = if_else((clean_text(title_cr) == clean_text(title_scholar) |
      safe_str_detect(clean_text(title_cr), clean_text(title_scholar)) |
      safe_str_detect(clean_text(title_scholar), clean_text(title_cr))), "y", "n"),
    cr_match_scholar = glue::glue("title: {toupper(title_match)} | year: {toupper(year_match)} | 1st: {toupper(first_match)}") %>%
      str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>")
  )


source("screen_matches.R")

# No doi means no possible match has been returned
scholar_results_merge$result[is.na(scholar_results_merge$doi)] <- "no_match_delete"

# Matches on year, first author and title don't need to be reviewed when title is long
# given that substrings are accepted, shorter titles could be false positives
# but if journal is the same, then this is also good
scholar_results_merge$result[((str_count(scholar_results_merge$title_scholar, "\\S+") >= 6 &
  str_count(scholar_results_merge$title_cr, "\\S+") >= 6) |
  (!is.na(scholar_results_merge$journal_scholar) & !is.na(scholar_results_merge$journal_cr) &
     clean_text(scholar_results_merge$journal_scholar) == clean_text(scholar_results_merge$journal_cr))) &
  map_int(
    str_extract_all(scholar_results_merge$cr_match_scholar, "Y"),
    length
  ) == 3] <- "match"



scholar_results_merge$Ys <- lengths(str_extract_all(scholar_results_merge$cr_match_scholar, "Y"))


scholar_results_merge[is.na(scholar_results_merge$result), ][1, ] %>% glimpse()

scholar_results_merge <- screen_matches(scholar_results_merge %>% arrange(-Ys, title_match, year_match, first_match), suffixes = c("scholar", "cr"))
```

#### Merge results

```{r}
scholar_results_merge <- scholar_results %>%
  select(id, result_id, type, resource_link, citation_count, citation_link) %>%
  right_join(scholar_results_merge)

matched <- scholar_results_merge %>%
  filter(result == "match") %>%
  transmute(
    database = "Google Scholar",
    doi = doi,
    gs_result_id = result_id,
    author = get_longer(author_scholar, author_cr),
    year = year_scholar,
    title = get_longer(title_scholar, title_cr),
    abstract = get_longer(abstract_scholar, abstract_cr),
    pubtype = type,
    pubtitle = get_longer(journal_scholar, journal_cr),
    resource_link,
    citation_count
  ) %>%
  tibble()

no_match <- scholar_results_merge %>%
  filter(str_detect(result, "no_match")) %>%
  transmute(
    database = "Google Scholar",
    doi = NA,
    gs_result_id = result_id,
    author = author_scholar,
    year = year_scholar,
    title = title_scholar,
    abstract = abstract_scholar,
    pubtype = NA,
    pubtitle = journal_scholar,
    resource_link,
    citation_count,
    citation_link
  ) %>%
  tibble()

cr_addition <- scholar_results_merge %>%
  filter(str_detect(result, "no_match_keep")) %>%
  transmute(
    database = "Crossref",
    doi = doi,
    author = author_cr,
    year = year_cr,
    title = title_cr,
    abstract = abstract_cr,
    pubtype = if_else(str_detect(journal_cr, fixed("journal", ignore_case = TRUE)), "journal-article", NA_character_),
    pubtitle = journal_cr
  ) %>%
  tibble()

scholar_final <- bind_rows_to_chr(matched, no_match, cr_addition)
write_rds(scholar_final, "results_final/scholar_final.RDS")
```

### For SSRN searches

```{r}
ssrn_results <- ssrn_results %>%
  tibble::rowid_to_column("id") %>%
  mutate(database = "SSRN")

ssrn_results_merge <-
  ssrn_results %>% transmute(
    id = id,
    doi = doi,
    cr_match_ssrn = cr_match %>% str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>"),
    cr_match_cr = "", # To show only once
    title_ssrn = title,
    title_cr = cr_title,
    author_ssrn = coalesce(select(., matches("author.*name")) %>%
      reduce(paste, sep = ", ") %>%
      str_remove_all(", NA,|NA,|, NA$| NA$") %>%
      str_trim() %>%
      str_replace("^$", NA_character_), first_author),
    author_cr = collapse_author_df(author),
    year_ssrn = year,
    year_cr = cr_year,
    abstract_ssrn = snippet,
    abstract_cr = abstract,
    journal_ssrn = str_extract(summary, "(?<= - ).*(?=, \\d)"), # Usually abbreviated
    journal_cr = journal
  )

ssrn_results_merge <- ssrn_results_merge %>%
  mutate(parse_result(cr_match_ssrn)) %>%
  mutate(
    title_match = if_else((clean_text(title_cr) == clean_text(title_ssrn) |
      safe_str_detect(clean_text(title_cr), clean_text(title_ssrn)) |
      safe_str_detect(clean_text(title_ssrn), clean_text(title_cr))), "y", "n"),
    cr_match_ssrn = glue::glue("title: {toupper(title_match)} | year: {toupper(year_match)} | 1st: {toupper(first_match)}") %>%
      str_replace_all("Y", "<span style='color:Darkgreen;font-weight: 900;'>Y</span>")
  )

source("screen_matches.R")

ssrn_results_merge$result[is.na(ssrn_results_merge$doi)] <- "no_match_delete"

# Matches on year, first author and title don't need to be reviewed when title is long
# given that substrings are accepted, shorter titles could be false positives
ssrn_results_merge$result[str_count(ssrn_results_merge$title_ssrn, "\\S+") >= 6 &
  str_count(ssrn_results_merge$title_cr, "\\S+") >= 6 &
  map_int(
    str_extract_all(ssrn_results_merge$cr_match_ssrn, "Y"),
    length
  ) == 3] <- "match"

# Sort by quality of matches so that "default" screening answer is consistent
ssrn_results_merge$Ys <- lengths(str_extract_all(ssrn_results_merge$cr_match_ssrn, "Y"))

ssrn_results_merge <- screen_matches(ssrn_results_merge %>% arrange(-Ys, title_match, year_match, first_match), suffixes = c("ssrn", "cr"))
```

#### Merge results

```{r}
ssrn_results_merge <- ssrn_results %>%
  select(id, result_id, type, resource_link, citation_count, citation_link) %>%
  right_join(ssrn_results_merge)

matched <- ssrn_results_merge %>%
  filter(result == "match") %>%
  transmute(
    database = "SSRN",
    doi,
    gs_result_id = result_id,
    author = get_longer(author_ssrn, author_cr),
    year = year_ssrn,
    title = get_longer(title_ssrn, title_cr),
    abstract = get_longer(abstract_ssrn, abstract_cr),
    pubtype = type,
    pubtitle = coalesce(journal_ssrn, journal_cr),
    resource_link,
    citation_count
  ) %>%
  tibble()

no_match <- ssrn_results_merge %>%
  filter(str_detect(result, "no_match")) %>%
  transmute(
    database = "SSRN",
    doi = NA,
    gs_result_id = result_id,
    author = author_ssrn,
    year = year_ssrn,
    title = title_ssrn,
    abstract = abstract_ssrn,
    pubtype = NA,
    pubtitle = journal_ssrn,
    resource_link,
    citation_count,
    citation_link
  ) %>%
  tibble()

cr_addition <- ssrn_results_merge %>%
  filter(str_detect(result, "no_match_keep")) %>%
  transmute(
    database = "Crossref",
    doi = doi,
    author = author_cr,
    year = year_cr,
    title = title_cr,
    abstract = abstract_cr,
    pubtype = if_else(str_detect(journal_cr, fixed("journal", ignore_case = TRUE)), "journal-article", NA_character_),
    pubtitle = journal_cr,
  ) %>%
  tibble()

ssrn_final <- bind_rows_to_chr(matched, no_match, cr_addition)
write_rds(ssrn_final, "results_final/ssrn_final.RDS")
```

## Merge results and remove duplicated DOIs

- DOIs are treated as definitive indices of duplication.

```{r}
full_results <- list(
  scholar_final,
  ssrn_final,
  psych_results,
  business_source_results,
  citation_metas_result,
  refs_previous_results,
  ndltd_results,
  open_diss_results
)

# Ensure that first has all columns - required for type conversion during merge
full_results[[1]][map(full_results, names) %>%
  unlist() %>%
  unique() %>%
  setdiff(names(full_results[[1]]))] <- NA

full_results <- full_results %>%
  do.call(bind_rows_to_chr, .) %>%
  mutate(record_id = as.character(row_number()))

# Merge columns
full_results <- full_results %>%
  mutate(
    type = case_when(
      type == "JOUR" ~ "journal-article",
      type == "THES" ~ "thesis_dissertation",
      type == "CHAP" ~ "chapter",
      type == "BOOK" ~ "book",
      type == "thesis_dissertation" ~ "thesis_dissertation",
      is.na(type) ~ NA_character_,
      TRUE ~ "other"
    ),
    pubtype = case_when(
      pubtype == "journal-article" ~ "journal-article",
      pubtype == "Article" ~ "journal-article",
      pubtype == "THES" ~ "thesis_dissertation",
      pubtype == "book-chapter" ~ "chapter",
      pubtype == "Book Chapter" ~ "chapter",
      pubtype == "book" ~ "book",
      pubtype == "Book" ~ "book",
      pubtype == "dissertation" ~ "thesis_dissertation",
      pubtype == "Review" ~ "review_article",
      is.na(pubtype) ~ NA_character_,
      TRUE ~ "other"
    ),
    pub_type = coalesce(type, pubtype)
  ) %>%
  unite(col = "journal", journal, pubtitle, na.rm = TRUE, sep = ": ") %>%
  transmute(record_id, database, title, author, year, journal,
    abstract_original = abstract,
    abstract = coalesce(abstract_english, abstract),
    doi, pub_type, database_detail = coalesce(cited_in, citing),
    volume, issue, start_page, end_page, publisher,
    resource_link,
    language, gs_result_id, scopus_id, citation_count, citation_link
  )

full_results <- full_results %>% mutate(
  doi = na_if(doi, ""),
  citation_count = list(tibble(source = database, citation_count = citation_count) %>%
    filter(!is.na(citation_count))),
  resource_link = list(tibble(source = database, resource_link = resource_link) %>%
    filter(!is.na(resource_link)))
)

full_results_no_doi <- full_results %>%
  filter(is.na(doi))

select_year <- function(...) {
  # Look for numeric year (in case one source was n.d.)
  years <- list(...) %>%
    unlist() %>%
    na.omit()
  if (length(years) == 0) {
    return(NA_character_)
  }
  years_num <- years %>%
    as.numeric() %>%
    coalesce()
  # Remove years that are invalid - more than 1 in the future, and pre-1700
  years_num[(years_num > as.integer(format(Sys.Date(), "%Y")) + 1) | years_num < 1700] <- NA
  if (!is.na(coalesce(!!!years_num))) {
    return(as.character(coalesce(!!!years_num)))
  }
  coalesce(!!!years)
}

full_results_doi <- full_results %>%
  filter(!is.na(doi)) %T>%
  {
    total_hits_with_doi <<- nrow(.)
  } %>%
  group_by(doi) %>%
  summarise(across(c(author, title, abstract, pubtype, pubtitle, journal), get_longest),
    across(c(citation_link, issue, volume, start_page, end_page, publisher, type), first),
    year = select_year(year),
    citation_count = list(bind_rows(citation_count)),
    resource_link = list(bind_rows(resource_link)),
    database = glue::glue_collapse(database, sep = "; "),
    record_id = glue::glue_collapse(record_id, sep = "; ")
  ) %>%
  ungroup()

print(glue::glue("Removed {total_hits_with_doi - nrow(full_results_doi)} results based on duplicate DOIs."))

full_results_post_doi <- bind_rows(full_results_no_doi, full_results_doi)
```

## Deduplicate other entries

```{r}
pacman::p_load_gh("camaradesuk/ASySD")

hits_dedup <- hits_post_doi %>%
  rename(number = issue) %>%
  mutate(pages = paste(start_page, end_page, sep = "-"), isbn = NA, label = NA, source = database) %>%
  dedup_citations()

hits_dedup %>% write_rds("results_dedup/automatic_dedup_done.Rds")
```

### Manually review possible duplicates

Here we again used a custom Shiny interface derived from [revtools](https://github.com/mjwestgate/revtools) code.

```{r}
source("helper-code/manual_dedup.R")

hits_dedup$manual_dedup <- hits_dedup$manual_dedup %>%
  rename_with(~ str_replace(.x, "1$", "_1")) %>%
  rename_with(~ str_replace(.x, "2$", "_2")) %>%
  manual_dedup()

# Merge those identified as duplicates (could also do with built-in ASySD function)
manual_matches <- hits_dedup$manual_dedup %>%
  filter(result == "match") %>%
  mutate(new_duplicate_id = (row_number() + nrow(hits)) %>%
    as.character()) %>%
  select(new_duplicate_id, record_id_1, record_id_2) %>%
  pivot_longer(2:3, names_to = NULL, values_to = "record_id")

hits_dedup$unique <- hits_dedup$unique %>% left_join(manual_matches)

already_unique <- hits_dedup$unique %>%
  filter(is.na(new_duplicate_id)) %>%
  rename(issue = number)

newly_unique <- hits_dedup$unique %>%
  filter(!is.na(new_duplicate_id)) %>%
  rename(issue = number) %>%
  group_by(new_duplicate_id) %>%
  summarise(across(c(author, title, abstract, pubtype, pubtitle, journal), get_longest),
    across(c(citation_link, issue, volume, start_page, end_page, publisher, source_type), ~ na.omit(unique(.x))[1]),
    year = select_year(year),
    citation_count = list(tibble(source = database, citation_count = citation_count) %>%
      filter(!is.na(citation_count))),
    resource_link = list(tibble(source = database, resource_link = resource_link) %>%
      filter(!is.na(resource_link))),
    database = glue::glue_collapse(database, sep = "; "),
    rowid = glue::glue_collapse(record_id, sep = "; ")
  ) %>%
  ungroup()

deduplicated_hits <- bind_rows(already_unique, newly_unique)

write_rds(deduplicated_hits, "results_dedup/deduplicated_hits.Rds")
```