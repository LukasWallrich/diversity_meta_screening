---
title: "Retrieving full-text"
author: "Lukas Wallrich"
date: "2023-04-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
source("helpers.R") #Helper functions - sometimes simple wrappers to deal safely with NULLs

```

## Collating results

### Merge main search back with metadata

```{r}
deduplicated_hits <- qs::qread("results_final/final_deduplicated_results.qs")
screened_hits <- read_csv("results_screened/asreview_main.csv")

included <- screened_hits %>% filter(included == 1) %>% 
  mutate(notes = coalesce(exported_notes_1, exported_notes_2, exported_notes_3, exported_notes_4)) %>% 
  select(-starts_with("exported_notes_"))

included %>% write_csv("results_screened/asreview_main_notes_merged.csv")

# Checked the merge - no discrepant titles
included_aug <- included %>% left_join(deduplicated_hits %>% select(duplicate_id, database = source, database_detail, 
                                                    year, journal, pub_type,
                                                    volume, issue, resource_link, 
                                                    gs_result_id, scopus_id, citation_count, citation_link
                                                    ))

# Merge two obvious duplicates identified in Excel
manual_merge <- deduplicated_hits %>% filter(author == "Griest, Debra Lynn") %>% 
    summarise(across(c(author, title, abstract, pub_type, journal), get_longest),
            across(c(citation_link, issue, volume, start_page, end_page, publisher, year), ~na.omit(unique(.x))[1]),
            citation_count = list(tibble(source = database, citation_count = citation_count) %>%
                                    filter(!is.na(citation_count))),
            resource_link = list(tibble(source = database, resource_link = resource_link) %>%
                                    filter(!is.na(resource_link))),
            source = glue::glue_collapse(database, sep = "; "),
            rowid = glue::glue_collapse(record_id, sep = "; ")) %>% transmute(duplicate_id = "-999", database = source,
                                                    year, journal, pub_type,
                                                    volume, issue, resource_link, 
                                                    citation_count, citation_link
                                                    )

included_aug <- included_aug %>% filter(!is.na(duplicate_id)) %>% 
  bind_rows(included %>% filter(is.na(duplicate_id)) %>% 
              select(-duplicate_id) %>% 
              bind_cols(manual_merge))

included_aug %>% qs::qsave("results_screened/asreview_main_augmented.qs")
```

### Retrieve initial round manually!

(could also have been done with scihub if that was legal and scihub.se still working - with something like the following code)

```{r}
for (i in 1:nrow(included_aug)) {
    if (!is.na(included_aug$doi[i])) {
    fn <- paste0("full_text/", str_replace(included_aug$doi[i], "/", "--"), ".pdf")
    if (!file.exists(fn)) {
           res <- possibly(scihubr::download_paper)(included_aug$doi[i], 
                              fn,  
                              open = FALSE)
               if (!file.exists(fn)) message("\nFail with ", included_aug$doi[i])

    }
    }
}
```

Keep track of what is retrieved

```{r}
dois_retrieved <- list.files("full_text/") %>% str_replace("--", "/") %>% str_remove(fixed(".pdf"))

included_aug <- included_aug %>% mutate(retrieved = if_else(doi %in% dois_retrieved, "pdf_downloaded", NA))
```

Retrieve from URLs

```{r}
urls <- included_aug %>% filter(is.na(retrieved)) %>% filter(map_int(resource_link, nrow) > 0) %>% 
  select(duplicate_id, resource_link)

urls <- map2_dfr(urls$duplicate_id, urls$resource_link, \(id, links) {
  if (is.character(links)) {
    out <- tibble(duplicate_id = id, resource_link = links)
  } else if (is.data.frame(links)) {
    if (is.character(links$resource_link)) {
      links$duplicate_id <- id
      out <- links  
    } else if (is.list(links$resource_link)) {
      out <- bind_rows(links$resource_link) %>% mutate(duplicate_id = id)
      if (is.list(out$resource_link)) {
              out <- bind_rows(out$resource_link) %>% mutate(duplicate_id = id)
      }
    } else {
    browser()
    }
  } else {
    browser()
  }
  out %>% mutate(resource_link = resource_link%>% str_remove(fixed('list("')) %>% str_remove(fixed('")')))
})

urls <- urls %>% filter(resource_link != "")

first_urls <- urls %>% arrange(-str_detect(resource_link, "pdf")) %>% 
  group_by(duplicate_id) %>% slice_head(n = 1) %>% ungroup()

first_urls %>% filter(str_detect(resource_link, "pdf")) %>% pwalk(\(...){
  current <- tibble(...)
  fn <- paste0("full_text/ID_", current$duplicate_id, ".pdf")
  try(download.file(current$resource_link, fn))
})

IDs_retrieved <- list.files("full_text/", pattern = "ID_") %>% str_remove("ID_") %>% str_remove(fixed(".pdf"))

included_aug <- included_aug %>% mutate(retrieved = if_else(duplicate_id %in% IDs_retrieved, "pdf_downloaded_resource_link", retrieved))
#This time preferentially select non-PDF links
remaining_urls <- urls %>% filter(!duplicate_id %in% IDs_retrieved) %>% 
  arrange(str_detect(resource_link, "pdf")) %>% 
  group_by(duplicate_id) %>% slice_head(n = 1) %>% ungroup()

remaining_urls <- remaining_urls %>% mutate(doi = str_extract(resource_link, "(10\\..+).*") %>% str_replace("%2F", "/") %>% 
  str_extract( "^(.*?/.*?)(?=\\?|&|/|$)"))

remaining_urls %>% select(-doi) %>% write_csv("results_final/urls.csv")

##
included_aug %>% mutate(filename = case_when(retrieved == "pdf_downloaded" ~ 
                                               paste0(str_replace(doi, "/", "--"), ".pdf"),
                                             retrieved == "pdf_downloaded_resource_link" ~
                                               paste0("ID_", duplicate_id, ".pdf")),
                        ID = coalesce(doi, duplicate_id)) %>% 
  filter(!is.na(retrieved)) %>% 
  transmute(ID, author, year, citation = CiteSource:::generate_apa_citation(author, year),
            title, doi, status = "", filename) %>% 
  write_csv("results_final/retrieved_hits.csv")



```


```{r}
library(rvest)

ndltd <- remaining_urls %>% filter(str_detect(resource_link, fixed("ndltd.org"))) %>% 
  mutate(resource_link = str_remove(resource_link, regex('", .*$', dotall = TRUE)) %>% 
           str_remove('list\\(c\\("'))

ndltd <- pmap_dfr(ndltd, \(...) {
  current <- tibble(...)
    # Load the webpage
webpage <- read_html(current$resource_link)

# Find the h5 tag with the text "Links & Downloads"
h5_tag <- webpage %>% 
  html_nodes("h5") %>% 
  html_text() %>% 
  grep("Links & Downloads", .)

# Check if the h5 tag is found
if (length(h5_tag) == 0) {
  cat("The h5 tag 'Links & Downloads' was not found.")
  current
} else {
  # Get the next div after the h5 tag
  next_div <- webpage %>%
    html_nodes(xpath = paste0("(//div/h5)[", h5_tag, "]/parent::div/parent::div"))
    # Extract the href of links in the next div
  links <- next_div %>% 
    html_nodes("a") %>% 
    html_attr("href")

  # Print the extracted links
  cat("Links extracted:\n")
  cat(paste(links, sep = "\n"))
  
  current$download_link <- links[1]
  current
}
  
})

remaining_urls <-ndltd %>% mutate(old = resource_link, resource_link = download_link, ndtld_link = old) %>% select(-old) %>% 
  bind_rows(remaining_urls %>% filter(!str_detect(resource_link, fixed("ndltd.org"))))

```


### Download from key open access sources

#### SSRN

```{r}

library(httr)

URL <- "https://doi.org/10.2139/ssrn.4119886" # Specify the DOI here
r <- GET(URL, followlocation = TRUE) # Redirects help follow to the actual domain

urls <- r$url

headers <- c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:100.0) Gecko/20100101 Firefox/100.0")

for (url in urls) {
    page <- read_html(url, headers=headers)
    pdf_url <- paste0("https://papers.ssrn.com/sol3/", html_attr(html_nodes(page, "a[data-abstract-id]"), "href"))
    filename <- paste0(strsplit(url, "=")[[1]][2], ".pdf")
    
    cat(sprintf("Downloading %s as %s\n", pdf_url, filename))
    
    download.file(pdf_url, destfile = filename, method = "auto", headers = headers, extra = add_headers("Referer" = url))
}
```



# Manually identified duplicates

```{r}
# Manual entries
  #     Superseded by 27781	84	26745
  #     Likely superseded by 24625 - confirm with authors	780	25688
  #     Available in 3567	80	26390

dupl <- read_csv("results_final/manual_dedup.csv")

dupl <- dupl %>% group_by(NEW_duplicate_ID) %>% 
  summarise(new_id = glue::glue_collapse(duplicate_id, "; ")) %>% 
  left_join(dupl) %>% select(-NEW_duplicate_ID)

included_aug <- included_aug %>% left_join(deduplicated_hits %>% select(duplicate_id, language)) %>% 
  left_join(dupl)

included_aug <- included_aug %>% mutate(ID = coalesce(doi, duplicate_id))

unique_incl <- included_aug %>% filter(is.na(new_id))

merged <- included_aug %>% filter(!is.na(new_id)) %>% group_by(new_id) %>% 
  summarise(res = sum(!is.na(retrieved))) %>% filter(res != 2) %>% select(-res) %>% left_join(included_aug) %>% 
  group_by(new_id) %>% 
        summarise(across(c(author, title, abstract, pub_type, journal), get_longest),
            across(c(citation_link, issue, volume, year, language), ~na.omit(unique(.x))[1]),
            citation_count = list(tibble(source = database, citation_count = citation_count) %>%
                                    filter(!is.na(citation_count))),
            resource_link = list(tibble(source = database, resource_link = resource_link) %>%
                                    filter(!is.na(resource_link))),
            database = glue::glue_collapse(database, sep = "; "),
            rowid = glue::glue_collapse(record_id, sep = "; "))


included_aug <- bind_rows(
  unique_incl, merged,
  included_aug %>% filter(!is.na(new_id)) %>% group_by(new_id) %>% 
  summarise(res = sum(!is.na(retrieved))) %>% filter(res == 2) %>% select(-res) %>% left_join(included_aug)
) %>% mutate(duplicate_id = coalesce(new_id, duplicate_id))
                                    
```


#### Rename downloads from collaborator

```{r}

names_in <- list.files("full_text/victoria/")

names_in %>% str_remove(" -.*$") %>% str_remove(" -.*$") %>% 
  str_remove(" \\(.*$") %>% str_replace_all("_ ", "; ") -> file_ids

map_chr(ids, ~included_aug$ID[included_aug$duplicate_id == .x]) %>% 

 {paste0("full_text/victoria/", str_replace(., "/", "--"), ".pdf")} -> files_to

file.rename(paste0("full_text/victoria/", names_in), files_to)

included_aug %>% filter(duplicate_id == "1706")


##
names_in <- list.files("full_text/victoria/")
names_in %>% str_remove(fixed(".pdf")) %>% str_replace_all("--", "/") -> IDs

included_aug$retrieved[included_aug$ID %in% IDs] <- "pdf_downloaded_victoria"


# Add URLs
urls_add <- read_csv("results_final/URL-additions.csv") %>% mutate(duplicate_id = as.character(duplicate_id))


urls <- map2_dfr(urls$duplicate_id, urls$resource_link, \(id, links) {
  if (is.character(links)) {
    out <- tibble(duplicate_id = id, resource_link = links)
  } else if (is.data.frame(links)) {
    if (is.character(links$resource_link)) {
      links$duplicate_id <- id
      out <- links  
    } else if (is.list(links$resource_link)) {
      out <- bind_rows(links$resource_link) %>% mutate(duplicate_id = id)
      if (is.list(out$resource_link)) {
              out <- bind_rows(out$resource_link) %>% mutate(duplicate_id = id)
      }
    } else {
    browser()
    }
  } else {
    browser()
  }
  out %>% mutate(resource_link = resource_link%>% str_remove(fixed('list("')) %>% str_remove(fixed('")')))
})

included_aug %>% left_join(urls_add) %>% mutate(resource_link = map_chr(resource_link, ~x$resource_link[1] %>% timesaveR::na_when(length(.) == 0)) %>% coalesce(URL, .)) %>% select(-URL) %>% 
  rename(Email = `Contact email`, download_status = Status) %>% count(is.na(resource_link))

included_aug <- urls %>% bind_rows(urls_add %>% rename(resource_link = URL)) %>% rename(url = resource_link) %>% left_join(included_aug, .)

included_aug <- included_aug %>% group_by(duplicate_id) %>% slice_head(n = 1) %>% ungroup()
  
  included_aug %>% qs::qsave("results_final/included_aug_post_victoria_dedup.qs")

included_aug %>% transmute(ID, title, abstract, doi, author, language, notes, included = NA, retrieved = !is.na(retrieved), url, Status, `Contact email`) %>% write_csv("results_final/input_victoria_2nd.csv", na = "")


```

