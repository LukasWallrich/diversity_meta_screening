---
title: "Search GS in other languages"
output:
  html_document:
    code_folding: hide
---

This process follows [the registration](https://osf.io/f5qdn) created after this meta-analysis was accepted in principle by the Journal of Business and Psychology.

# Packages used and set-up

```{r}
if (!require(pacman)) install.packages("pacman")
pacman::p_load(
  reticulate, tidyr, dplyr, jsonlite, purrr, stringr, rcrossref, bib2df, furrr,
  shiny, shinythemes, progress, progressr, keys, shinyjs, readr, synthesisr,
  rvest, glue, magrittr, qs
)

pacman::p_load_gh("gadenbuie/shinyThings")

# Enable progress updates
# handlers(global = TRUE) <- this needs to be run in the console for now
handlers(handler_progress(
  format = ":spin :current/:total (:message) [:bar] :percent in :elapsed (remaining: :eta)",
  width = 60,
  complete = "=",
  clear = FALSE
))

options(progressr.clear = FALSE)


# Adds progress bars to map and future_map functions
showing_progress <- function(x, fn, fn2, ..., steps = length(x)) {
  if (interactive() && !handlers(global = NA)) stop("To use this function, first run `handlers(global = TRUE)` in the console")
  p <- progressr::progressor(steps = steps)
  p(amount = 0)
  fn(x, function(...) {
    out <- fn2(...)
    p()
    out
  })
}


set.seed(1234)

# Import
source("helper-code/API_keys.R")
source("helper-code/crossref_and_related.R")
source("helper-code/helpers.R")

source_python("helper-code/get_scrape_functions.py")

library(magrittr)
```


## Searches in other languages

### Run Google Scholar search per language

See SM 1 for details - including the need to split cyrillic languages into multiple search strings.

```{r search_strings}
single_strings <- c(
  german = '(Diversity OR Diversität OR heterogen OR Heterogenität OR Teamzusammensetzung OR "individuelle Unterschiede") (Team OR Gruppe) (Leistung OR Kreativität OR Produktivität OR Innovation OR Effektivität)',
  chinese = "(多种的 OR 多样性 OR 异构性 OR 异构的 OR 团队构成 OR 个体差异) (团队 OR 小组) (表现 OR 绩效 OR 创造力 OR 生产力 OR 绩效 OR 创新)",
  spanish = '(diverso OR diversidad OR heterogéneo OR heterogeneidad OR "composición del equipo" OR "diferencias individuales") AND (equipo OR grupo) AND (desempeño OR creatividad OR productividad OR innovación OR eficacia)',
  italian = '(diverso OR diversità OR eterogeno OR "composizione del team" OR "differenze individuali") AND (team OR gruppo) AND (prestazione OR creatività OR produttività OR innovazione)',
  polish = '(różnorodny OR różnorodność OR niejednolitość OR niejednolity OR "skład zespołu" OR "różnice indywidualne") AND (zespół OR grupa) AND (wydajność OR kreatywność OR produktywność OR innowacja)',
  japanese = '(多様性 OR  異質 OR "チーム構成" OR個人差) AND (チーム OR 集団) AND ("パフォーマンス" OR創造性 OR生産性 OR 技術革新 OR 有用性)',
  korean = "(다양성 OR 이질성 OR 팀의구성) AND (팀 OR그룹) AND (성과 OR 팀수행에미 OR창의성 OR 생산성 OR 혁신 OR 효과성)",
  portuguese = '(diversificado OR diversidade OR heterogeneo OR heterogeneidade OR "composição da equipa" OR "diferenças individuais") AND (equipa OR grupo) AND (desempenho OR criatividade OR produtividade OR inovação OR eficácia)',
  french = '(divers OR diversité OR hétérogène OR hétérogénéité OR "composition d\'équipe" OR "différences individuelles") AND (équipe OR groupe) AND (performance OR créativité OR productivité OR innovation OR efficacité)',
  indonesian = '(keragaman OR keanekaragaman OR heterogen OR "komposisi tim" OR "perbedaan individu") AND (tim OR regu) AND (kinerja OR prestasi OR kreativitas OR produktivitas OR inovasi OR keefektifan)'
)

single_strings_lang <- c(
  german = "de",
  chinese = "zh-cn",
  spanish = "es",
  italian = "it",
  polish = "pl",
  japanese = "ja",
  korean = "ko",
  portuguese = "pt",
  french = "fr",
  indonesian = "id"
)

multiple_strings <- list(
  ukrainian = c(
    "(різноманітністю OR гетерогенність OR неоднорідність) AND (команда OR група) AND (продуктивність OR креативність OR продуктивність)",
    "(різноманітністю OR гетерогенність OR неоднорідність) AND (команда OR група) AND (інноваційність OR ефективність)",
    '("склад команди" OR "індивідуальні відмінності" OR diversity) AND (команда OR група) AND (продуктивність OR креативність OR продуктивність)',
    '("склад команди" OR "індивідуальні відмінності" OR diversity) AND (команда OR група) AND (інноваційність OR ефективність)'
  ),
  russian = c(
    '(разнообразная OR diversity OR неоднородная OR неоднородность OR "состав команды") AND (команда OR группа) AND (производительность OR инновация)',
    '(разнообразная OR diversity OR неоднородная OR неоднородность OR "состав команды") AND (команда OR группа) AND (творчество OR креативность)',
    '(разнообразная OR diversity OR неоднородная OR неоднородность OR "состав команды") AND (команда OR группа) AND (инновации OR эффективность)',
    '(разнообразная OR diversity OR неоднородная OR неоднородность OR "состав команды") AND (команда OR группа) AND (продуктивность OR результативность)',
    '(разнообразие OR "индивидуальные различия" OR "индивидуальные особенности") AND (команда OR группа) AND (производительность OR инновация)',
    '(разнообразие OR "индивидуальные различия" OR "индивидуальные особенности") AND (команда OR группа) AND (творчество OR креативность)',
    '(разнообразие OR "индивидуальные различия" OR "индивидуальные особенности") AND (команда OR группа) AND (инновации OR эффективность)',
    '(разнообразие OR "индивидуальные различия" OR "индивидуальные особенности") AND (команда OR группа) AND (продуктивность OR результативность)'
  )
)

multiple_strings_lang <- c(
  russian = "ru",
  ukrainian = "uk"
)
```

#### Retrieve results

Using serpapi.

```{r}
language_results <- map2(single_strings, single_strings_lang, \(search_string, lang) {
  language_results <- get_scholar_results(
    q = search_string,
    max_results = 1000, serp_key = serp_key,
    lang = lang
  )
})

language_results_russian <- map(multiple_strings$russian, \(search_string) {
  language_results <- get_scholar_results(
    q = search_string,
    max_results = 1.2 * 1000 / length(multiple_strings$russian), # 20% extra for duplication between search strings
    serp_key = serp_key,
    lang = multiple_strings_lang["russian"]
  )
})

language_results_russian_df <- language_results_russian %>%
  bind_rows_to_list(!!!.) %>%
  mutate(language = "russian") %>%
  group_by(result_id) %>%
  slice_head(n = 1) %>%
  ungroup()

language_results_ukrainian <- map(multiple_strings$ukrainian, \(search_string) {
  language_results <- get_scholar_results(
    q = search_string,
    max_results = 1.1 * 1000 / length(multiple_strings$ukrainian), # 10% extra for duplication between search strings
    serp_key = serp_key,
    lang = multiple_strings_lang["ukrainian"]
  )
})

language_results_ukrainian_df <- language_results_ukrainian %>%
  bind_rows_to_list(!!!.) %>%
  mutate(language = "ukrainian") %>%
  group_by(result_id) %>%
  slice_head(n = 1) %>%
  ungroup()

# For single-string languages
language_results_df <- language_results %>% bind_rows_to_list(!!!., .id = "language")

# For UKR/RU

language_results_df <- bind_rows(language_results_russian_df, language_results_ukrainian_df)

language_results_df <- language_results_df %>%
  unnest_wider(publication_info) %>%
  select(-type) %>%
  unnest_wider(authors, names_sep = "_", names_repair = "universal") %>%
  unnest_wider(matches("authors_[0-9]+$"), names_sep = "_", names_repair = "unique") %>% # Unnests all columns matching the term
  mutate(
    year = str_extract(summary, "[0-9]{4}"),
    first_author = str_remove(summary, ",.*$") %>%
      str_remove("-.*$") %>% str_remove("^[A-Z]* ")
  ) %>%
  hoist(resources, resource_link = c(1, 3)) %>%
  hoist(inline_links, citation_count = c("cited_by", "total"), citation_link = "serpapi_cite_link")

extract_authors_from_summary <- function(summary) {
  map(summary, ~ {
    out <-
      .x %>%
      word(sep = " - ") %>%
      str_split_1(", ") %>%
      as.list() %>%
      set_names(paste0("authors_", seq_len(length(.)), "_name")) %>%
      data.frame()
    if (nrow(out) != 1) browser()
    out
  }) %>% bind_rows()
}

# Add missing authors from summary (sometimes, authors are not returned separately, but always included in snippet)
language_results_df <- language_results_df %>%
  filter(is.na(authors_1_name)) %>%
  mutate(extract_authors_from_summary(summary)) %>%
  bind_rows(language_results_df %>% filter(!is.na(authors_1_name)))

# Remove duplicate results
language_results_df <- language_results_df %>% distinct(result_id, .keep_all = TRUE)

write_rds(language_results_df, glue::glue("results_aligned/scholar_non-English-cyrillic.RDS"))
```

#### Translate

Used Google Translate for initial translation as registered, except for cyrillic languages, where GPT-3.5 was used, as Google Translate results were of poor quality (likely due to lack of context and use of incomplete sentences in snippets).

```{r}
language_results_df <- read_rds("results_aligned/scholar_non-English-cyrillic.RDS")

pacman::p_load(googleLanguageR, openai)

# OpenAI API sometimes times out or fails - so retry up to 4 times
insistent_request <- possibly(insistently(create_chat_completion), otherwise = list(), quiet = FALSE)

gl_auth("google_translate.json")

translate <- function(x, drop_tags = TRUE, warn = TRUE, engine = c("gl_translate", "gpt")) {
  if (warn && sum(str_length(x), na.rm = TRUE) > 5000) {
    if (usethis::ui_nope(glue::glue("Translation of {sum(str_length(x))} characters requested. Continue?"))) {
      return(NULL)
    }
  }

  if (drop_tags) {
    x <- str_replace_all(x, "<.*?>", " ")
  }
  which_NA <- is.na(x)
  x <- c(na.omit(x))

  if (engine[1] == "gl_translate") {
    t <- gl_translate(x)
    t %>%
      count(detectedSourceLanguage) %>%
      transmute(N = paste0(detectedSourceLanguage, ": ", n)) %>%
      pull() %>%
      glue::glue_collapse(sep = ", ", last = "&") %>%
      message("Detected languages: ", .)

    out <- character(length(which_NA))
    out[!which_NA] <- t %>% pull(translatedText)
    out[which_NA] <- NA
  } else if (engine[1] == "gpt") {
    future::plan(multisession, workers = 6)


    t <- furrr::future_map_chr(x, .progress = TRUE, \(current){
      current_resp <-
        insistent_request(
          model = "gpt-3.5-turbo",
          messages = list(
            list(
              "role" = "system",
              "content" = "You are a translator focused on management and psychology"
            ),
            list(
              "role" = "user",
              "content" = paste(
                "Translate to English:
",
                current
              )
            )
          )
        )

      r <- current_resp$choices$message.content

      if (is.null(r)) r <- NA

      r
    })

    out <- character(length(which_NA))
    out[!which_NA] <- t
    out[which_NA] <- NA
  } else {
    stop("Should not be here")
  }

  out
}

translate_longer_PDF <- function(f) {
  # Split the PDF into 20-page files
  f %>%
    normalizePath() %>%
    {
      glue::glue('qpdf --split-pages=20 "{.}" {dirname(.)}/output-%d.pdf')
    } %>%
    system()

  dir.create(file.path(dirname(f), "translated"))

  # Translate the split files
  list.files(path = dirname(f), pattern = "output-\\d+-\\d+\\.pdf", full.names = TRUE) %>%
    walk(~ {
      gl_translate_document(., target = "en", source = "", output_path = file.path("translated", basename(.)))
    })

  # Combine translated files into one
  combined_output <- file.path("translated", basename(f)) %>% normalizePath()
  glue::glue('qpdf --empty --pages "translated/output-*.pdf" -- "{combined_output}"') %>%
    system()
}
```


```{r}
language_results_list <- language_results_df %>% split(.$language)

language_results_list <- imap(language_results_list, \(df, lang) {
  message("Translating ", lang)
  df %>%
    mutate(across(c(title, snippet), ~ translate(.x, warn = FALSE), .names = "{.col}_english")) %>%
    rename(
      title_original = title, snippet_original = snippet,
      title = title_english, snippet = snippet_english
    )
})
```

#### Process Russian & Ukrainian

```{r}
# Select 1000 per language for consistency
set.seed(1234)
language_results_df <- language_results_df %>%
  group_by(language) %>%
  sample_n(1000) %>%
  ungroup()

language_results_df <- language_results_df %>%
  select(-ends_with("english")) %>%
  mutate(across(c(title, snippet),
    ~ translate(.x, engine = "gpt", warn = FALSE),
    .names = "{.col}_english"
  ))

# Don't add abstracts here so that entries remain more even - let's see if that is better
language_results_df %>%
  mutate(journal = summary %>% str_remove("^.*- ") %>% str_remove(" -.*$")) %>%
  split(.$language) %>%
  iwalk(\(df, lang){
    df %>%
      transmute(result_id,
        title = title_english, abstract = paste0(journal, "\n\n", snippet_english),
        url = unlist(link)
      ) %>%
      write_csv(glue("results_final/scholar_results_{lang}.csv"))
  })
```

## Add abstracts

Screening works much better with full abstracts - so we tried to scrape those from webpages that hosted many of the search results. 

```{r}
unlist_nan <- function(x) {
  x[map_lgl(x, is.nan)] <- NA
  unlist(x)
}

# Frequent hosts of webpage links
language_results_df %>%
  mutate(link = unlist_nan(link) %>% str_remove("#.*$")) %>%
  filter(!str_detect(link, "\\.pdf$")) %>%
  mutate(domain = link %>% str_remove(".*?\\.") %>% str_remove("/.*")) %>%
  group_by(domain) %>%
  filter(n() >= 20) %>%
  slice_head(n = 2) %>%
  select(domain, link) %>%
  gt::gt()

# Frequent hosts of PDF links
language_results_df %>%
  mutate(link = unlist_nan(link) %>% str_remove("#.*$")) %>%
  filter(str_detect(link, "\\.pdf$")) %>%
  mutate(domain = link %>% str_remove(".*?\\.") %>% str_remove("/.*")) %>%
  group_by(domain) %>%
  filter(n() >= 20) %>%
  slice_head(n = 2) %>%
  select(domain, link) %>%
  gt::gt()
```

rivisteweb.it and torossa.com do not seem to have abstracts

For PDFs, academia.edu links appear broken, redalyc.org does not have abstracts

```{r}
library(rvest)

pdf_workarounds <- c("core.ac.uk", "ac.uk", "bibliotekanauki.pl", ".pl", "koreascience.or.kr", "researchgate.net", "springer.com")

# Identify (using Chrome's "Inspect" function) which tags contain abstracts
abstracts_tags_specific <- tibble::tribble(
  ~target_domain, ~tag, ~class, ~note,
  "cairn.info", "div", "resume lang-en", NA,
  "ceeol.com", "p", "summary", NA,
  "cqvip.com", "td", "sum", NA,
  "dbpia.co.kr", "div", "abstractTxt", NA,
  "jstage.jst.go.jp", "div", "article-overiew-abstract-wrap", "id, not class",
  "scielo.br", "article", "articleText", "id, not class",
  "repo.nii.ac.jp", "td", "td_detail_line_repos w80", "then get longest - these are all meta-data fields", # Some links there are also direct download links,
  "springer.com", "div", "Abs1-section", NA,
  "bibliotekanauki.pl", "p", "abstract-text preserve-whitespace", "if two exist, second likely to be the English one",
  "core.ac.uk", "section", "abstract", "id",
  "koreascience.org.kr", "div", "article-box", NA,
  "researchgate.net", "div", "section__abstract", NA,
  "sciencedirect.com", "div", "abstract author", "2nd likely English, if there are 2",
  "icm.edu.pl", "div", "articleDetails-abstract", "2nd likely English",
  "unirioja.es", "ul", "resumen", NA,
  "jstor.org", "p", "summary-paragraph", NA,
  "google.com", "div", "synopsistext", "id"
)

abstract_names <- c("abstract", "resume")
frequent_classes <- c("div", "td", "p")

generic_selector <- map(frequent_classes, ~ glue("{.x}[class*='{abstract_names}'], {.x}[id*='{abstract_names}']")) %>%
  unlist() %>%
  paste(collapse = ", ")

uastring <- "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36"

httr::set_config(httr::user_agent(uastring))

scrape_abstract <- function(url) {
  url <- url %>% str_remove("#.*$")
  domain <- url %>%
    str_remove(".*?\\.") %>%
    str_remove("/.*")

  if (str_detect(url, "\\.pdf$")) {
    if (!domain %in% pdf_workarounds) {
      return(tibble(url = url, domain = domain, skipped = "pdf"))
    } else {
      if (domain %in% c("bibliotekanauki.pl", "koreascience.or.kr", ".pl")) {
        url <- str_remove(url, "\\.pdf$")
      } else if (domain == "core.ac.uk" || domain == "ac.uk") {
        url <- str_replace(url, "/download/pdf/", "/display/") %>%
          str_remove("\\.pdf$")
      } else if (domain == "researchgate.net") {
        url <- str_extract(url, "^.*publication/.*?/")
      } else if (domain == "springer.com") {
        url <- paste0("https://doi.org/", str_extract(url, "10\\..*$") %>% str_remove("\\.pdf"))
      }
      scrape_abstract(url)
    }
  }

  if (domain == "kjjb.org" & str_detect(url, "downloadArticleFile.do")) {
    # Transform download URLs into links to abstracts
    url <- url %>%
      str_extract("id=.*$") %>%
      str_remove("id=") %>%
      {
        glue("http://www.kjjb.org/CN/abstract/abstract{.}.shtml")
      }
  } else if (domain == "scielo.br") {
    url <- url %>% str_replace("/citation/", "/abstract/")
  } else if (str_detect(url, "books.google.com")) {
    url <- url %>% str_remove("&oi.*")
  }

  if (str_detect(url, "create_pdf|downloadArticleFile\\.do|(bitstream.*\\.pdf)")) {
    cli::cli_alert_danger("Link appears to be a download link. Skipping.")
    cli::cli_alert_info(paste("URL: ", url))
    return(tibble(url = url, domain = domain, skipped = "pdf"))
  }

  rate <- rate_backoff(pause_base = 4, max_times = 4)

  url_data <- (possibly(insistently(read_html, rate), NULL))(url)

  if (is.null(url_data)) {
    cli::cli_alert_danger("URL could not be read. Skipping.")
    cli::cli_alert_info(paste("URL: ", url))
    return(tibble(url = url, domain = domain, skipped = "httperror"))
  }

  if (domain == "psych.ac.cn") {
    out <- html_nodes(url_data, "p") %>%
      html_text() %>%
      stringi::stri_enc_toutf8() %>%
      str_subset("Abstract:")
    return(tibble(url = url, domain = domain, abstract = out))
  }

  if (domain == "um.ac.id") {
    out <- html_nodes(url_data, "div[id='articleAbstract']") %>%
      html_text() %>%
      stringi::stri_enc_toutf8()
    if (length(out) > 0) {
      return(tibble(url = url, domain = domain, abstract = out))
    }
    out <- html_nodes(url_data, "div[class='ep_summary_content_main']") %>%
      html_nodes("p") %>%
      html_text() %>%
      stringi::stri_enc_toutf8() %>%
      .[2]
    return(tibble(url = url, domain = domain, abstract = out))
  }

  if (domain == "upv.es") {
    out <- html_nodes(url_data, "tr") %>%
      html_text() %>%
      stringi::stri_enc_toutf8() %>%
      str_subset("Abstract|Resumen")
    return(tibble(url = url, domain = domain, abstract = out))
  }

  different <- FALSE
  tentative <- FALSE

  if (domain %in% abstracts_tags_specific$target_domain) {
    details <- abstracts_tags_specific %>% filter(target_domain == domain)
    selector <- glue::glue("{details$tag}[id*='{details$class}'], {details$tag}[class*='{details$class}']")
  } else {
    different <- TRUE
    selector <- generic_selector
  }
  # Use CSS selectors to extract the text from the div
  out <- div_text <- html_nodes(url_data, selector) %>%
    html_text() %>%
    stringi::stri_enc_toutf8() %>%
    str_squish_mild() %>% # Added after initial run!
    get_longest()

  if (length(out) == 0 | is.na(out)) {
    # As last resort, find the longest paragraph on the page
    divs <- html_nodes(url_data, "div")
    texts <- divs[sapply(divs, function(x) length(html_nodes(x, "div")) == 0)] %>%
      html_text()

    out <- c(texts, html_nodes(url_data, "p, tr") %>% html_text()) %>%
      stringi::stri_enc_toutf8() %>%
      str_squish_mild() %>% # Added after initial run!
      get_longest()

    tentative <- TRUE
  }
  tibble(
    url = url, domain = domain, abstract = out,
    different_source = different, tentative_extraction = tentative
  )
}

plan(multisession, workers = 7)

# Randomly sort links to space out calls to pages in particular language
links <- language_results_df$link %>%
  unlist() %>%
  sample()
```

Retrieve abstracts - might need to split this into a few attempts (1000 links at a time seemed to work robustly)
Currently retries all URLs up to 4 times - needed for e.g. Researchgate - but could filter so that only some errors are retried to speed this up

```{r}
scraped <- showing_progress(links %>% set_names(links), future_map, possibly(scrape_abstract, tibble()))
scraped_df <- scraped %>% bind_rows(.id = "link")
# Some links will fail due to rate limits - so worth trying once more
failed_links <- scraped_df %>%
  filter(skipped == "httperror") %>%
  pull(url)
scraped2 <- showing_progress(failed_links %>% set_names(failed_links), future_map, possibly(scrape_abstract, tibble()))

scraped_df <- bind_rows(scraped, scraped2, .id = "link") %>%
  arrange(url, is.na(abstract)) %>%
  group_by(url) %>%
  slice_head(n = 1) %>%
  ungroup()

language_results_df <- language_results_df %>%
  mutate(link = unlist(link)) %>%
  left_join(
    by = "link",
    scraped_df %>% select(link, abstract,
      tentative_extraction_abstract = tentative_extraction
    )
  )
```

Further abstracts may be available in Crossref - so searched in line with process for English results

```{r}
cr_search_cols <- language_results_df %>%
  filter(is.na(abstract) | tentative_extraction_abstract) %>%
  select(result_id, title, year, authors_1_name, summary)

job::job(title = "Augment GS crossref", {
  handlers(global = TRUE)
  handlers(handler_progress(
    format = ":spin :current/:total (:message) [:bar] :percent in :elapsed (remaining: :eta)",
    width = 60,
    complete = "=",
    clear = FALSE
  ))
  plan(multisession, workers = 5) # Parallel API requests to crossref
  cr_search_cols <- cr_search_cols %>% showing_progress(function(...) future_pmap_dfr(..., .options = furrr_options(seed = TRUE)), function(...) {
    pacman::p_load(dplyr, stringr, tidyr, rcrossref)
    source("helper-code/crossref_and_related.R") # Functions to retrieve and edit citations
    source("helper-code/helpers.R") # Helper functions - sometimes simple wrappers to deal safely with NULLs
    df <- extract_crossref_try(tibble(...))
    df
  }, steps = nrow(.))
})

# Only treat those where authors, titles AND year match as Crossref matches
# Then retrieve those abstracts

cr_search_cols$Ys <- str_count(cr_search_cols$cr_match, "Y")

cr_search_cols_abs <- cr_search_cols %>%
  filter(Ys == 3) %>%
  rowwise() %>%
  mutate(abstract = (possibly(cr_abstract, NA, quiet = FALSE))(doi))

language_results_df <- cr_search_cols_abs %>%
  select(result_id, doi, abstract_cr = abstract) %>%
  right_join(language_results_df)

language_results_df <- language_results_df %>% mutate(
  abstract = coalesce(abstract_cr, abstract),
  tentative_extraction_abstract = case_when(
    !is.na(abstract_cr) ~ FALSE,
    TRUE ~ tentative_extraction_abstract
  )
)
```


Some "abstracts" scraped contain reference lists - these mislead screening algorithm, so need to be removed.

```{r}
remove_ref_list <- function(string) {
  str_remove(string, regex("\\. \\([0-9]{4}\\)((, )|( [\"“]{1})).*\\. \\([0-9]{4}\\)((, )|( [\"“]{1}))", dotall = TRUE))
}
language_results_df <- language_results_df %>%
  mutate(abstract_old = abstract, abstract = remove_ref_list(abstract))
```

Extract English segments of multi-lingual abstract.

```{r}
language_results_df <- language_results_df %>%
  rowwise() %>%
  mutate(abstract_english = extract_english(abstract)) %>%
  ungroup()
```

Use GPT3 model to translate "Western" languages - currently costs <10% of Google Translate (in alphabetic languages)
Google Translate is cheaper (and seems better) for ideographic languages, as it charges per character, not token

For Russian/Ukrainian, use GPT 3.5 (ChatGPT), which is substantially better (https://blog.inten.to/chatgpt-for-translation-surpassing-gpt-3-e8e34f37befb) - though much slower than the simpler GPT-3 models.


```{r}
pacman::p_load(googleLanguageR)
gl_auth("google_translate.json")

# For comparison between GPT3 and Google Translate, keep track of what costs would be
total_cost <- 0
google_translate_cost <- 0

translate_if_needed <- function(text_english, text, language) {
  if (is.na(text) || text == "") {
    return(NA)
  }
  if (!is.na(text_english) && (str_length(text_english) > 200 || str_length(text) < 2 * str_length(text_english))) {
    return(text_english)
  }

  if (str_length(text) > 1600) {
    text <- paste(str_sub(text, end = 1000), "...")
  }

  if (language %in% c("chinese", "japanese", "korean")) {
    t <- gl_translate(text)
    t %>% pull(translatedText)
  } else {
    res <- openai::create_completion(
      model = "text-curie-001",
      prompt = paste("Translate this to English: ", text, sep = "\n"),
      max_tokens = floor(str_length(text) / 4),
      openai_api_key = Sys.getenv("OPENAI_API_KEY"),
      temperature = .5
    )
    total_cost <<- total_cost + res$usage$total_tokens / 1000 * .002
    google_translate_cost <<- google_translate_cost + str_length(text) / 1000 * .02
    return(res$choices$text)
  }
}

abstracts_english <- character()
id <- character()

# Using a for loop to retain WIP in case of errors
for (i in cli::cli_progress_along(seq_len(nrow(language_results_df)), "Translating")) {
  abstracts_english[i] <- translate_if_needed(
    language_results_df$abstract_english[i],
    language_results_df$abstract[i],
    language_results_df$language[i]
  )
  id[i] <- language_results_df$result_id[i]
}

language_results_df$abstract_english <- coalesce(abstracts_english, language_results_df$abstract_english)
```

## Export for abstract screening

To screen abstracts, we split them by language given that ASReview would likely pick up different keywords between the languages.

```{r}
language_results_df <- language_results_df %>% mutate(journal = summary %>% str_remove("^.*- ") %>% str_remove(" -.*$"))

language_results_list <- language_results_df %>% split(.$language)

iwalk(language_results_list, \(df, lang){
  df %>%
    transmute(result_id, title,
      abstract = paste0(journal, "\n\n", snippet, "\n\n", abstract_english),
      url = unlist(link)
    ) %>%
    write_csv(glue("results_final/scholar_results_{lang}.csv"))
})
```

Registered to screen a minimum of 25% in each language. Given that there were internal duplications in the Google Scholar results, this resulted in a different number for each language.

```{r}
imap(language_results_list, \(df, lang) {
  tibble(language = lang, share_en_abs = mean(!is.na(df$abstract_english)), hits = nrow(df), screen_min = ceiling(hits * .25))
}) %>%
  bind_rows() %>%
  gt::gt() %>%
  gt::fmt_percent(2, decimals = 0) %>%
  gt::tab_header("Results after internal deduplication")
```
